{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:black; font-size:120%; text-align:left;padding:3.0px; background: #cceeff; border-bottom: 8px solid #004466\" > TABLE OF CONTENTS<br><div>  \n* [IMPORTS](#1)\n* [INTRODUCTION](#2)\n* [DATA PROCESSING](#3)\n* [MODEL TRAINING](#4) \n* [MODEL INFERENCING](#5) \n* [OUTRO](#6)  \n ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> PACKAGE IMPORTS<br><div> ","metadata":{}},{"cell_type":"code","source":"%%time \n\n# General library imports:-\nfrom IPython.display import display_html, clear_output, Markdown;\nfrom gc import collect;\n\nfrom copy import deepcopy;\nimport pandas as pd;\nimport numpy as np;\nimport joblib;\nfrom os import system, getpid, walk;\nfrom psutil import Process;\nimport ctypes;\nlibc = ctypes.CDLL(\"libc.so.6\");\n\nfrom pprint import pprint;\nfrom colorama import Fore, Style, init;\nfrom warnings import filterwarnings;\nfilterwarnings('ignore');\n\nfrom tqdm.notebook import tqdm;\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:30.407545Z","iopub.execute_input":"2023-10-31T16:04:30.407967Z","iopub.status.idle":"2023-10-31T16:04:31.064312Z","shell.execute_reply.started":"2023-10-31T16:04:30.407933Z","shell.execute_reply":"2023-10-31T16:04:31.062902Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\nCPU times: user 545 ms, sys: 42.6 ms, total: 588 ms\nWall time: 612 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\n# Model development:-\nfrom sklearn.model_selection import (RepeatedStratifiedKFold as RSKF, \n                                     StratifiedKFold as SKF,\n                                     KFold, \n                                     RepeatedKFold as RKF, \n                                     cross_val_score);\n\nfrom lightgbm import log_evaluation, early_stopping, LGBMRegressor as LGBMR;\nfrom xgboost import XGBRegressor as XGBR;\nfrom catboost import CatBoostRegressor as CBR;\nfrom sklearn.ensemble import HistGradientBoostingRegressor as HGBR;\nfrom sklearn.metrics import mean_absolute_error as mae, make_scorer;\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:31.066139Z","iopub.execute_input":"2023-10-31T16:04:31.066626Z","iopub.status.idle":"2023-10-31T16:04:34.139562Z","shell.execute_reply.started":"2023-10-31T16:04:31.066594Z","shell.execute_reply":"2023-10-31T16:04:34.138185Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\nCPU times: user 1.95 s, sys: 473 ms, total: 2.43 s\nWall time: 3.07 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n# Defining global configurations and functions:-\n\n# Color printing    \ndef PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    \"Prints color outputs using colorama using a text F-string\";\n    print(style + color + text + Style.RESET_ALL); \n    \ndef GetMemUsage():\n    \"\"\"\n    This function defines the memory usage across the kernel. \n    Source-\n    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n    \"\"\";\n    \n    pid = getpid();\n    py = Process(pid);\n    memory_use = py.memory_info()[0] / 2. ** 30;\n    return f\"RAM memory GB usage = {memory_use :.4}\";\n\n# Making sklearn pipeline outputs as dataframe:-\nfrom sklearn import set_config; \nset_config(transform_output = \"pandas\");\npd.set_option('display.max_columns', 50);\npd.set_option('display.max_rows', 50);\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.140982Z","iopub.execute_input":"2023-10-31T16:04:34.141354Z","iopub.status.idle":"2023-10-31T16:04:34.254279Z","shell.execute_reply.started":"2023-10-31T16:04:34.141324Z","shell.execute_reply":"2023-10-31T16:04:34.253284Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nCPU times: user 104 ms, sys: 85 µs, total: 104 ms\nWall time: 104 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> INTRODUCTION<br><div> ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n1. This notebook is my first tryst with the Optiver challenge. This is a time series regression problem involving stock market trading data at the day's close auction book. <b>Mean Absolute Error metric</b> is used here <br>\n2. This notebook aims to train a baseline model using a simple CV strategy from the memory reduced datasets created for the challenge. <br>\n3. This is a continuation from my baseline data curation notebook and dataset. We continue the analysis herewith and train models to elicit a CV score. We then infer using these models here and make a submission<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size: 90%; text-align:left;padding:4.0px; background: maroon; border-bottom: 5px solid black\"> VERSION DETAILS<br><div> ","metadata":{}},{"cell_type":"markdown","source":"| Version<br>Number | Version<br>Details | Preparation <br> date|LGBMR <br> CV|CBR <br> CV| XGBR <br> CV| HGBR <br> CV|Best LB <br>score|Single/<br> Ensemble|\n| :-: | --- | :-: |  :-: |:-: |:-: |:-: |:-: |:-: |\n|V1| * Baseline features <br> * No null treatments and scaling <br> * Simple ML models without tuning <br> * 5x1 K-fold CV <br> * Simple weighted ensemble| 22Sep2023|6.248286|6.25538|6.27198|6.266826|5.3702| Ensemble <br> LGBMR CBR|\n|V2| * Baseline features <br> * No null treatments and scaling <br> * Simple ML models without tuning with altered parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble| 23Sep2023|6.23334|6.2535|||5.3728| Ensemble <br> LGBMR CBR|\n|V3| * Baseline features <br> * No null treatments and scaling <br> * ML models with V1 parameters <br> * 5x3 Repeated K-fold CV <br> * Simple weighted ensemble| 24Sep2023|6.248288| 6.25532||6.267036|5.3712|Ensemble <br> LGBMR CBR |\n|V4| * Baseline features + **Median volume new feature** <br> * No null treatments and scaling <br> * ML models with V1 parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble **with goto conversion**| 01Oct2023|6.241901|6.250738 |||5.3638| Ensemble <br> LGBMR CBR |\n|V5| * Used my dataset as input instead of kernel output <br> * Baseline features + **Median volume new features** <br> * ML models with V1 parameters <br> * 5x1 K-fold CV <br> * Simple weighted ensemble **with goto conversion**| 02Oct2023|6.239849|6.250021 ||6.262478|5.3635| Ensemble <br> LGBMR CBR |","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n## <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size: 90%; text-align:left;padding:4.0px; background: maroon; border-bottom: 5px solid black\"> CONFIGURATION PARAMETERS<br><div> ","metadata":{}},{"cell_type":"markdown","source":"| Parameter | Comments | Sample values|\n| :-: | --- | :-: |\n|version_nb | Version Number| integer value|\n|test_req| Are we testing the code?| Y/N|\n|test_frac| Test fraction for sampling and testing <br> Place small values for easy execution| float between 0 and 1|\n|load_tr_data| Are we loading the train data here? <br> If we are inferring only, this is not required | Y/N|\n|gpu_switch| Do we need a GPU here? |Y/N|\n|state| Random seed| integer|\n|target| Target column name| string value|\n|path| Data path for model training <br> I point this to my baseline data curation kernel| |\n|test_path| Relevant path for test data| Competition artefacts|\n|df_choice| Which data do I need for analysis? <br> Refer the baseline data prep kernel for details ||\n|mdl_path| Path to dump trained models with joblib||\n|inf_path| Appropriate path to extract the models for inference <br> I point to my baseline dataset with models trained as a starter||\n|methods| All trained model methods, choose 1-more based on the memory constraints <br> For inferencing, all trained methods need to be present|list |\n|ML| Do we need to do model training here? |Y/N |\n|n_splits| CV number of splits |integer value|\n|n_repeats| CV number of repetitions |integer value|\n|nbrnd_erly_stp| Number of early stopping rounds|integer value|\n|mdlcv_mthd| Model CV choice |KF, SKF, RSKF, RKF|\n|ensemble_req| Do we need an ensemble here? <br> Currently this is unused |Y/N|\n|enscv_mthd| Ensemble CV choice- used mostly with Optuna |KF, SKF, RSKF, RKF|\n|metric_obj| Based on the metric, do we wish to maximize/ minimize the function? |maximize/ minimize|\n|ntrials| Number of Optuna trials |integer value|\n|ens_weights| Weights if decided subjecively |list<br> apropos to number of trained methods|\n|inference_req| Do we need to infer here? |Y/N|","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Configuration class:-\nclass CFG:\n    \"\"\"\n    Configuration class for parameters and CV strategy for tuning and training\n    Please use caps lock capital letters while filling in parameters\n    \"\"\";\n    \n    # Data preparation:-   \n    version_nb         = 5;\n    test_req           = \"N\";\n    test_frac          = 0.01;\n    load_tr_data       = \"N\";\n    gpu_switch         = \"OFF\"; \n    state              = 42;\n    target             = 'target';\n    \n    path               = f\"/kaggle/input/optiver-memoryreduceddatasets/\";\n    test_path          = f\"/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\";\n    df_choice          = f\"XTrIntCmpNewFtre.parquet\";\n    mdl_path           = f'/kaggle/working/BaselineML/';\n    inf_path           = f'/kaggle/input/optiverbaselinemodels/';\n     \n    # Model Training:-\n    methods            = [\"LGBMR\", \"CBR\", \"HGBR\"];\n    ML                 = \"N\";\n    n_splits           = 5;\n    n_repeats          = 1;\n    nbrnd_erly_stp     = 100 ;\n    mdlcv_mthd         = 'KF';\n    \n    # Ensemble:-    \n    ensemble_req       = \"N\";\n    enscv_mthd         = \"KF\";\n    metric_obj         = 'minimize';\n    ntrials            = 10 if test_req == \"Y\" else 200;\n    ens_weights        = [0.54, 0.44, 0.02];\n    \n    # Inference:-\n    inference_req      = \"Y\";\n    \n    # Global variables for plotting:-\n    grid_specs = {'visible': True, 'which': 'both', 'linestyle': '--', \n                  'color': 'lightgrey', 'linewidth': 0.75\n                 };\n    title_specs = {'fontsize': 9, 'fontweight': 'bold', 'color': 'tab:blue'};\n\nprint();\nPrintColor(f\"--> Configuration done!\\n\");\ncollect();\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.256731Z","iopub.execute_input":"2023-10-31T16:04:34.257656Z","iopub.status.idle":"2023-10-31T16:04:34.387600Z","shell.execute_reply.started":"2023-10-31T16:04:34.257590Z","shell.execute_reply":"2023-10-31T16:04:34.386572Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n\u001b[1m\u001b[34m--> Configuration done!\n\u001b[0m\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.2539\u001b[0m\nCPU times: user 106 ms, sys: 1.19 ms, total: 107 ms\nWall time: 106 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\n# Commonly used CV strategies for later usage:-\nall_cv= {'KF'  : KFold(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state),\n         'RKF' : RKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n         'RSKF': RSKF(n_splits= CFG.n_splits, n_repeats = CFG.n_repeats, random_state= CFG.state),\n         'SKF' : SKF(n_splits= CFG.n_splits, shuffle = True, random_state= CFG.state)\n        };\n\n# Defining the competition metric:-\ndef ScoreMetric(ytrue, ypred)-> float:\n    \"\"\"\n    This function calculates the metric for the competition. \n    ytrue- ground truth array\n    ypred- predictions\n    returns - metric value (float)\n    \"\"\";\n    \n    return mae(ytrue, ypred);\n\n# Designing a custom scorer to use in cross_val_predict and cross_val_score:-\nmyscorer = make_scorer(ScoreMetric, greater_is_better = False, needs_proba=False,);\n\nprint();\ncollect();\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.388817Z","iopub.execute_input":"2023-10-31T16:04:34.389517Z","iopub.status.idle":"2023-10-31T16:04:34.510442Z","shell.execute_reply.started":"2023-10-31T16:04:34.389484Z","shell.execute_reply":"2023-10-31T16:04:34.509517Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.2539\u001b[0m\nCPU times: user 105 ms, sys: 102 µs, total: 105 ms\nWall time: 104 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\ndef goto_conversion(listOfOdds, total = 1, eps = 1e-6, isAmericanOdds = False):\n    \"Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\";\n\n    #Convert American Odds to Decimal Odds\n    if isAmericanOdds:\n        for i in range(len(listOfOdds)):\n            currOdds = listOfOdds[i];\n            isNegativeAmericanOdds = currOdds < 0;\n            if isNegativeAmericanOdds:\n                currDecimalOdds = 1 + (100/(currOdds*-1));\n            else: \n                #Is non-negative American Odds\n                currDecimalOdds = 1 + (currOdds/100);\n            listOfOdds[i] = currDecimalOdds;\n\n    #Error Catchers\n    if len(listOfOdds) < 2:\n        raise ValueError('len(listOfOdds) must be >= 2');\n    if any(x < 1 for x in listOfOdds):\n        raise ValueError('All odds must be >= 1, set isAmericanOdds parameter to True if using American Odds');\n\n    #Computation:-\n    #initialize probabilities using inverse odds\n    listOfProbabilities = [1/x for x in listOfOdds];\n    \n    #compute the standard error (SE) for each probability\n    listOfSe = [pow((x-x**2)/x,0.5) for x in listOfProbabilities];\n    \n    #compute how many steps of SE the probabilities should step back by\n    step = (sum(listOfProbabilities) - total)/sum(listOfSe) ;\n    outputListOfProbabilities = [min(max(x - (y*step),eps),1) for x,y in zip(listOfProbabilities, listOfSe)];\n    return outputListOfProbabilities;\n\ndef zero_sum(listOfPrices, listOfVolumes):\n    \"\"\"\n    Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\n    \"\"\";\n    \n    #compute standard errors assuming standard deviation is same for all stocks\n    listOfSe = [x**0.5 for x in listOfVolumes];\n    step = sum(listOfPrices)/sum(listOfSe);\n    outputListOfPrices = [x - (y*step) for x,y in zip(listOfPrices, listOfSe)];\n    return outputListOfPrices;\n\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.512237Z","iopub.execute_input":"2023-10-31T16:04:34.512647Z","iopub.status.idle":"2023-10-31T16:04:34.647423Z","shell.execute_reply.started":"2023-10-31T16:04:34.512615Z","shell.execute_reply":"2023-10-31T16:04:34.645972Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 107 ms, sys: 46 µs, total: 107 ms\nWall time: 107 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n<b>How to use these kernels</b> <br>\n1. Use the memory reduction kernel input to curate features, reduce dataset memory and prepare essential datasets as input to this kernel. Else, use the starter dataset as input if features are already ready. Links are provided below. <br>\n<b> Baseline input features:-</b> https://www.kaggle.com/code/ravi20076/optiver-memoryreduction<br>\n<b> Baseline input dataset:-</b> https://www.kaggle.com/datasets/ravi20076/optiver-memoryreduceddatasets<br>\n2. Design one's model framework here for a baseline and train models. It is advisable to train 1/2 models at a time to prevent memory overflow issues <br>\n3. Store the model objects in the BaselineML directory in the working folder for inferencing <br>\n4. It is advisable to infer and submit separately. This will surely not create a data memory overlow problem. In this case, please turn off training and do not load the training dataset. In this case, I have stored the model training artefacts in the link- https://www.kaggle.com/datasets/ravi20076/optiverbaselinemodels <br>\n5. While inferencing, make sure to curate the same features as used in the training process. I shall make an improvement here and update the kernel shortly. <br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> DATA PROCESSING<br><div> ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\nIn this version, we choose the int-float compressed dataset with new features as per the reference notebook <br>\n</div>","metadata":{}},{"cell_type":"code","source":"%%time \n\nif (CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\") and CFG.test_req == \"Y\":\n    if isinstance(CFG.test_frac, float):\n        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(frac = CFG.test_frac);\n    else:\n        X = pd.read_parquet(CFG.path + CFG.df_choice).sample(n = CFG.test_frac);\n        \n    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").loc[X.index].squeeze();\n    PrintColor(f\"---> Sampled train shapes for code testing = {X.shape} {y.shape}\", \n               color = Fore.RED);\n    X.index, y.index = range(len(X)), range(len(y));\n    \n    PrintColor(f\"\\n---> Train set columns for model development\");\n    pprint(X.columns, width = 100, depth = 1, indent = 5);\n    print();\n\nelif CFG.load_tr_data == \"Y\" or CFG.ML == \"Y\":\n    X = pd.read_parquet(CFG.path + CFG.df_choice);\n    y = pd.read_parquet(CFG.path + f\"Ytrain.parquet\").squeeze();  \n    PrintColor(f\"---> Train shapes for code testing = {X.shape} {y.shape}\");\n\nelif CFG.load_tr_data != \"Y\" or CFG.inference_req == \"Y\":\n    PrintColor(f\"---> Train data is not required as we are infering from the model\");\n    \nprint();\ncollect();\nlibc.malloc_trim(0);\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.651952Z","iopub.execute_input":"2023-10-31T16:04:34.652451Z","iopub.status.idle":"2023-10-31T16:04:34.804406Z","shell.execute_reply.started":"2023-10-31T16:04:34.652406Z","shell.execute_reply":"2023-10-31T16:04:34.803210Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[34m---> Train data is not required as we are infering from the model\u001b[0m\n\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.2524\u001b[0m\nCPU times: user 124 ms, sys: 106 µs, total: 124 ms\nWall time: 123 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> MODEL TRAINING AND CV<br><div> ","metadata":{}},{"cell_type":"code","source":"%%time \n\n# Initializing model I-O:-\n\nif CFG.ML == \"Y\":\n    Mdl_Master = \\\n    {'CBR': CBR(**{'task_type'           : \"GPU\" if CFG.gpu_switch == \"ON\" else \"CPU\",\n                   'objective'           : \"MAE\",\n                   'eval_metric'         : \"MAE\",\n                   'bagging_temperature' : 0.5,\n                   'colsample_bylevel'   : 0.7,\n                   'iterations'          : 500,\n                   'learning_rate'       : 0.065,\n                   'od_wait'             : 25,\n                   'max_depth'           : 7,\n                   'l2_leaf_reg'         : 1.5,\n                   'min_data_in_leaf'    : 1000,\n                   'random_strength'     : 0.65, \n                   'verbose'             : 0,\n                   'use_best_model'      : True,\n                  }\n               ), \n\n      'LGBMR': LGBMR(**{'device'            : \"gpu\" if CFG.gpu_switch == \"ON\" else \"cpu\",\n                        'objective'         : 'regression_l1',\n                        'boosting_type'     : 'gbdt',\n                        'random_state'      : CFG.state,\n                        'colsample_bytree'  : 0.7,\n                        'subsample'         : 0.65,\n                        'learning_rate'     : 0.065,\n                        'max_depth'         : 6,\n                        'n_estimators'      : 500,\n                        'num_leaves'        : 150,  \n                        'reg_alpha'         : 0.01,\n                        'reg_lambda'        : 3.25,\n                        'verbose'           : -1,\n                       }\n                    ),\n\n      'XGBR': XGBR(**{'tree_method'        : \"gpu_hist\" if CFG.gpu_switch == \"ON\" else \"hist\",\n                      'objective'          : 'reg:absoluteerror',\n                      'random_state'       : CFG.state,\n                      'colsample_bytree'   : 0.7,\n                      'learning_rate'      : 0.07,\n                      'max_depth'          : 6,\n                      'n_estimators'       : 500,                         \n                      'reg_alpha'          : 0.025,\n                      'reg_lambda'         : 1.75,\n                      'min_child_weight'   : 1000,\n                      'early_stopping_rounds' : CFG.nbrnd_erly_stp,\n                     }\n                  ),\n\n      \"HGBR\" : HGBR(loss              = 'squared_error',\n                    learning_rate     = 0.075,\n                    early_stopping    = True,\n                    max_iter          = 200,\n                    max_depth         = 6,\n                    min_samples_leaf  = 1500,\n                    l2_regularization = 1.75,\n                    scoring           = myscorer,\n                    random_state      = CFG.state,\n                   )\n    };\n\nprint();\ncollect();\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);\n","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.806697Z","iopub.execute_input":"2023-10-31T16:04:34.807293Z","iopub.status.idle":"2023-10-31T16:04:34.934127Z","shell.execute_reply.started":"2023-10-31T16:04:34.807239Z","shell.execute_reply":"2023-10-31T16:04:34.933048Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.2526\u001b[0m\nCPU times: user 110 ms, sys: 2.24 ms, total: 112 ms\nWall time: 112 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\nif CFG.ML == \"Y\":\n    # Initializing the models from configuration class:-\n    methods = CFG.methods;\n\n    # Initializing a folder to store the trained and fitted models:-\n    system('mkdir BaselineML');\n\n    # Initializing the model path for storage:-\n    model_path = CFG.mdl_path;\n\n    # Initializing the cv object:-\n    cv = all_cv[CFG.mdlcv_mthd];\n        \n    # Initializing score dataframe:-\n    Scores = pd.DataFrame(index = range(CFG.n_splits * CFG.n_repeats),\n                          columns = methods).fillna(0).astype(np.float32);\n    \n    FtreImp = pd.DataFrame(index = X.columns, columns = [methods]).fillna(0);\n\nprint();\ncollect();\nlibc.malloc_trim(0);\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED);","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:34.935663Z","iopub.execute_input":"2023-10-31T16:04:34.935995Z","iopub.status.idle":"2023-10-31T16:04:35.054807Z","shell.execute_reply.started":"2023-10-31T16:04:34.935964Z","shell.execute_reply":"2023-10-31T16:04:35.053168Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.2524\u001b[0m\nCPU times: user 106 ms, sys: 2.86 ms, total: 109 ms\nWall time: 109 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\nif CFG.ML == \"Y\":\n    PrintColor(f\"\\n{'=' * 25} ML Training {'=' * 25}\\n\");\n    \n    # Initializing CV splitting:-       \n    for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y)), \n                                              f\"{CFG.mdlcv_mthd} CV {CFG.n_splits}x{CFG.n_repeats}\"\n                                             ): \n        # Creating the cv folds:-    \n        Xtr  = X.iloc[train_idx];   \n        Xdev = X.iloc[dev_idx];\n        ytr  = y.iloc[train_idx];\n        ydev = y.iloc[dev_idx];\n        \n        PrintColor(f\"-------> Fold{fold_nb} <-------\");\n        # Fitting the models:- \n        for method in methods:\n            model = Mdl_Master[method];\n            if method == \"LGBMR\":\n                model.fit(Xtr, ytr, \n                          eval_set = [(Xdev, ydev)], \n                          verbose = 0, \n                          eval_metric = \"mae\",\n                          callbacks = [log_evaluation(0,), \n                                       early_stopping(CFG.nbrnd_erly_stp, verbose = False)], \n                         );\n\n            elif method == \"XGBR\":\n                model.fit(Xtr, ytr, \n                          eval_set = [(Xdev, ydev)], \n                          verbose = 0, \n                          eval_metric = \"mae\",\n                         );  \n\n            elif method == \"CBR\":\n                model.fit(Xtr, ytr, \n                          eval_set = [(Xdev, ydev)], \n                          verbose = 0, \n                          early_stopping_rounds = CFG.nbrnd_erly_stp,\n                         ); \n\n            else:\n                model.fit(Xtr, ytr);\n\n            #  Saving the model for later usage:-\n            joblib.dump(model, CFG.mdl_path + f'{method}V{CFG.version_nb}Fold{fold_nb}.model');\n            \n            # Creating OOF scores:-\n            score = ScoreMetric(ydev, model.predict(Xdev));\n            Scores.at[fold_nb, method] = score;\n            num_space = 6- len(method);\n            PrintColor(f\"---> {method} {' '* num_space} OOF = {score:.5f}\", \n                       color = Fore.MAGENTA);  \n            del num_space, score;\n            \n            # Collecting feature importances:-\n            try:\n                FtreImp[method] = \\\n                FtreImp[method].values + (model.feature_importances_ / (CFG.n_splits * CFG.n_repeats));\n            except:\n                pass;\n            \n            collect();\n            \n        PrintColor(GetMemUsage());\n        print();\n        del Xtr, ytr, Xdev, ydev;\n        collect();\n    \n    clear_output();\n    PrintColor(f\"\\n---> OOF scores across methods <---\\n\");\n    Scores.index.name = \"FoldNb\";\n    Scores.index = Scores.index + 1;\n    display(Scores.style.format(precision = 5).\\\n            background_gradient(cmap = \"Pastel1\")\n           );\n    \n    PrintColor(f\"\\n---> Mean OOF scores across methods <---\\n\");\n    display(Scores.mean());\n    \n    try: FtreImp.to_csv(CFG.mdl_path + f\"FtreImp_V{CFG.version_nb}.csv\");\n    except: pass;\n        \ncollect();\nprint();\nlibc.malloc_trim(0);\n\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.GREEN);","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:35.059845Z","iopub.execute_input":"2023-10-31T16:04:35.060340Z","iopub.status.idle":"2023-10-31T16:04:35.193324Z","shell.execute_reply.started":"2023-10-31T16:04:35.060299Z","shell.execute_reply":"2023-10-31T16:04:35.192179Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n\u001b[1m\u001b[32m\nRAM memory GB usage = 0.2525\u001b[0m\nCPU times: user 109 ms, sys: 1.87 ms, total: 111 ms\nWall time: 110 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> MODEL INFERENCING AND SUBMISSION<br><div> ","metadata":{}},{"cell_type":"code","source":"%%time \n\ndef MakeFtre(df : pd.DataFrame, prices: list) -> pd.DataFrame:\n    \"\"\"\n    This function creates new features using the price columns. This was used in a baseline notebook as below-\n    https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost\n    \n    Inputs-\n    df:- pd.DataFrame -- input dataframe\n    cols:- price columns for transformation\n    \n    Returns-\n    df:- pd.DataFrame -- dataframe with extra columns\n    \"\"\";\n    \n    features = ['overall_medvol', \"first5min_medvol\", \"last5min_medvol\",\n                'seconds_in_bucket', 'imbalance_buy_sell_flag',\n                'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n                'imb_s1', 'imb_s2'\n               ];\n    \n    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)').astype(np.float32);\n    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)').astype(np.float32);\n       \n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            if i>j:\n                df[f'{a}_{b}_imb'] = df.eval(f'({a}-{b})/({a}+{b})');\n                features.append(f'{a}_{b}_imb'); \n                    \n    for i,a in enumerate(prices):\n        for j,b in enumerate(prices):\n            for k,c in enumerate(prices):\n                if i>j and j>k:\n                    max_ = df[[a,b,c]].max(axis=1);\n                    min_ = df[[a,b,c]].min(axis=1);\n                    mid_ = df[[a,b,c]].sum(axis=1)-min_-max_;\n\n                    df[f'{a}_{b}_{c}_imb2'] = ((max_-mid_)/(mid_-min_)).astype(np.float32);\n                    features.append(f'{a}_{b}_{c}_imb2');\n    \n    return df[features];\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:35.194851Z","iopub.execute_input":"2023-10-31T16:04:35.195893Z","iopub.status.idle":"2023-10-31T16:04:35.318201Z","shell.execute_reply.started":"2023-10-31T16:04:35.195850Z","shell.execute_reply":"2023-10-31T16:04:35.317043Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\nCPU times: user 107 ms, sys: 0 ns, total: 107 ms\nWall time: 107 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\n# Creating the testing environment:-\nif CFG.inference_req == \"Y\":\n    try: \n        del X, y;\n    except: \n        pass;\n        \n    prices = ['reference_price', 'far_price', 'near_price', 'bid_price', 'ask_price', 'wap'];\n    \n    # Making the test environment for inferencing:-\n    import optiver2023;\n    try: \n        env = optiver2023.make_env();\n        iter_test = env.iter_test();\n        PrintColor(f\"\\n---> Curating the inference environment\");\n    except: \n        pass;\n    \n    # Collating a list of models to be used for inferencing:-\n    models = [];\n\n    # Loading the models for inferencing:-\n    if CFG.ML != \"Y\": \n        model_path = CFG.inf_path;\n        PrintColor(f\"---> Loading models from the input data for the kernel - V{CFG.version_nb}\\n\", \n                  color = Fore.RED);\n    elif CFG.ML == \"Y\": \n        model_path = CFG.mdl_path;\n        PrintColor(f\"---> Loading models from the working directory for the kernel\\n\");\n    \n    # Loading the models from the models dataframe:-\n    mdl_lbl = [];\n    for _, _, filename in walk(model_path):\n        mdl_lbl.extend(filename);\n\n    models = [];\n    for filename in mdl_lbl:\n        models.append(joblib.load(model_path + f\"{filename}\"));\n        \n    mdl_lbl    = [m.replace(r\".model\", \"\") for m in mdl_lbl];\n    model_dict = {l:m for l,m in zip(mdl_lbl, models)};\n    PrintColor(f\"\\n---> Trained models\\n\");    \n    pprint(np.array(mdl_lbl), width = 100, indent = 10, depth = 1);  \n       \nprint();\ncollect();  \nlibc.malloc_trim(0);\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); ","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:04:35.319737Z","iopub.execute_input":"2023-10-31T16:04:35.320796Z","iopub.status.idle":"2023-10-31T16:06:06.373831Z","shell.execute_reply.started":"2023-10-31T16:04:35.320753Z","shell.execute_reply":"2023-10-31T16:06:06.372544Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[34m\n---> Curating the inference environment\u001b[0m\n\u001b[1m\u001b[31m---> Loading models from the input data for the kernel - V5\n\u001b[0m\n\u001b[1m\u001b[34m\n---> Trained models\n\u001b[0m\narray(['CBRV2Fold2', 'CBRV3Fold5', 'LGBMRV3Fold4', 'CBRV5Fold3',\n       'LGBMRV2Fold3', 'HGBRFold0', 'HGBRFold1', 'CBRV3Fold8',\n       'LGBMRV3Fold1', 'CBRV5Fold1', 'CBRV3Fold6', 'LGBMRFold3',\n       'CBRV4Fold1', 'LGBMRV5Fold0', 'HGBRFold4', 'HGBRV5Fold4',\n       'LGBMRFold1', 'CBRV4Fold3', 'LGBMRV3Fold3', 'LGBMRV3Fold0',\n       'CBRV3Fold4', 'CBRV2Fold0', 'XGBRFold2', 'LGBMRV3Fold7',\n       'HGBRFold3', 'CBRV3Fold13', 'CBRV4Fold2', 'CBRV2Fold3',\n       'CBRV3Fold1', 'LGBMRFold4', 'LGBMRV5Fold4', 'LGBMRV4Fold0',\n       'CBRV2Fold1', 'LGBMRV3Fold8', 'CBRV3Fold7', 'LGBMRV3Fold2',\n       'LGBMRV5Fold3', 'CBRV4Fold4', 'CBRV3Fold10', 'HGBRV5Fold1',\n       'CBRV5Fold4', 'CBRFold0', 'LGBMRV3Fold10', 'LGBMRV2Fold4',\n       'LGBMRV3Fold11', 'CBRFold1', 'LGBMRFold2', 'LGBMRV2Fold0',\n       'LGBMRV3Fold12', 'LGBMRV4Fold2', 'LGBMRV5Fold1', 'CBRV3Fold12',\n       'CBRFold3', 'XGBRFold4', 'CBRV3Fold0', 'CBRFold2', 'CBRV5Fold2',\n       'LGBMRV2Fold1', 'HGBRV5Fold0', 'CBRV3Fold2', 'LGBMRV3Fold5',\n       'CBRV3Fold3', 'LGBMRV3Fold9', 'LGBMRV3Fold14', 'LGBMRFold0',\n       'LGBMRV4Fold3', 'LGBMRV5Fold2', 'CBRV3Fold14', 'XGBRFold3',\n       'CBRV3Fold11', 'CBRV2Fold4', 'LGBMRV4Fold4', 'XGBRFold1',\n       'CBRV4Fold0', 'HGBRFold2', 'LGBMRV4Fold1', 'HGBRV5Fold3',\n       'XGBRFold0', 'CBRFold4', 'LGBMRV3Fold13', 'LGBMRV3Fold6',\n       'LGBMRV2Fold2', 'CBRV5Fold0', 'HGBRV5Fold2', 'CBRV3Fold9'],\n      dtype='<U13')\n\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.4925\u001b[0m\nCPU times: user 1min 26s, sys: 931 ms, total: 1min 27s\nWall time: 1min 31s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time \n\nif CFG.inference_req == \"Y\":\n    print();\n    counter = 0;\n    \n    try:\n        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\", index_col = ['Unnamed: 0']);\n    except:\n        median_vol = pd.read_csv(CFG.path + f\"MedianVolV2.csv\"); \n    median_vol.index.name = \"stock_id\";\n    median_vol = median_vol[['overall_medvol', \"first5min_medvol\", \"last5min_medvol\"]];\n    \n    for test, revealed_targets, sample_prediction in iter_test:\n        if counter >= 99: num_space = 1;\n        elif counter >= 9: num_space = 2;\n        else: num_space = 3;\n        \n        PrintColor(f\"{counter + 1}. {' ' * num_space} Inference\", color = Fore.MAGENTA);\n        test  = test.merge(median_vol, how = \"left\", left_on = \"stock_id\", right_index = True);\n        Xtest = MakeFtre(test, prices = prices);\n        del num_space;\n        \n        # Curating model predictions across methods and folds:-        \n        preds = pd.DataFrame(columns = CFG.methods, index = Xtest.index).fillna(0);\n        for method in CFG.methods:\n            for mdl_lbl, mdl in model_dict.items():\n                if mdl_lbl.startswith(f\"{method}V{CFG.version_nb}\"):\n                    if CFG.test_req == \"Y\":\n                        print(mdl_lbl);\n                    else:\n                        pass;\n                    preds[method] = preds[method] + mdl.predict(Xtest)/ (CFG.n_splits * CFG.n_repeats);\n        \n        # Curating the weighted average model predictions:-       \n        sample_prediction['target'] = \\\n        np.average(preds.values, weights= CFG.ens_weights, axis=1);\n        \n        # Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models     \n        sample_prediction['target'] = \\\n        zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n        \n        try: \n            env.predict(sample_prediction);\n        except: \n            PrintColor(f\"---> Submission did not happen as we have the file already\");\n            pass;\n        \n        counter = counter+1;\n        collect();\n    \n    PrintColor(f\"\\n---> Submission file\\n\");\n    display(sample_prediction.head(10));\n            \nprint();\ncollect();  \nlibc.malloc_trim(0);\nPrintColor(f\"\\n\" + GetMemUsage(), color = Fore.RED); ","metadata":{"execution":{"iopub.status.busy":"2023-10-31T16:06:06.375728Z","iopub.execute_input":"2023-10-31T16:06:06.376162Z","iopub.status.idle":"2023-10-31T16:07:09.431642Z","shell.execute_reply.started":"2023-10-31T16:06:06.376121Z","shell.execute_reply":"2023-10-31T16:07:09.430360Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nThis version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n\u001b[1m\u001b[35m1.     Inference\u001b[0m\n\u001b[1m\u001b[35m2.     Inference\u001b[0m\n\u001b[1m\u001b[35m3.     Inference\u001b[0m\n\u001b[1m\u001b[35m4.     Inference\u001b[0m\n\u001b[1m\u001b[35m5.     Inference\u001b[0m\n\u001b[1m\u001b[35m6.     Inference\u001b[0m\n\u001b[1m\u001b[35m7.     Inference\u001b[0m\n\u001b[1m\u001b[35m8.     Inference\u001b[0m\n\u001b[1m\u001b[35m9.     Inference\u001b[0m\n\u001b[1m\u001b[35m10.    Inference\u001b[0m\n\u001b[1m\u001b[35m11.    Inference\u001b[0m\n\u001b[1m\u001b[35m12.    Inference\u001b[0m\n\u001b[1m\u001b[35m13.    Inference\u001b[0m\n\u001b[1m\u001b[35m14.    Inference\u001b[0m\n\u001b[1m\u001b[35m15.    Inference\u001b[0m\n\u001b[1m\u001b[35m16.    Inference\u001b[0m\n\u001b[1m\u001b[35m17.    Inference\u001b[0m\n\u001b[1m\u001b[35m18.    Inference\u001b[0m\n\u001b[1m\u001b[35m19.    Inference\u001b[0m\n\u001b[1m\u001b[35m20.    Inference\u001b[0m\n\u001b[1m\u001b[35m21.    Inference\u001b[0m\n\u001b[1m\u001b[35m22.    Inference\u001b[0m\n\u001b[1m\u001b[35m23.    Inference\u001b[0m\n\u001b[1m\u001b[35m24.    Inference\u001b[0m\n\u001b[1m\u001b[35m25.    Inference\u001b[0m\n\u001b[1m\u001b[35m26.    Inference\u001b[0m\n\u001b[1m\u001b[35m27.    Inference\u001b[0m\n\u001b[1m\u001b[35m28.    Inference\u001b[0m\n\u001b[1m\u001b[35m29.    Inference\u001b[0m\n\u001b[1m\u001b[35m30.    Inference\u001b[0m\n\u001b[1m\u001b[35m31.    Inference\u001b[0m\n\u001b[1m\u001b[35m32.    Inference\u001b[0m\n\u001b[1m\u001b[35m33.    Inference\u001b[0m\n\u001b[1m\u001b[35m34.    Inference\u001b[0m\n\u001b[1m\u001b[35m35.    Inference\u001b[0m\n\u001b[1m\u001b[35m36.    Inference\u001b[0m\n\u001b[1m\u001b[35m37.    Inference\u001b[0m\n\u001b[1m\u001b[35m38.    Inference\u001b[0m\n\u001b[1m\u001b[35m39.    Inference\u001b[0m\n\u001b[1m\u001b[35m40.    Inference\u001b[0m\n\u001b[1m\u001b[35m41.    Inference\u001b[0m\n\u001b[1m\u001b[35m42.    Inference\u001b[0m\n\u001b[1m\u001b[35m43.    Inference\u001b[0m\n\u001b[1m\u001b[35m44.    Inference\u001b[0m\n\u001b[1m\u001b[35m45.    Inference\u001b[0m\n\u001b[1m\u001b[35m46.    Inference\u001b[0m\n\u001b[1m\u001b[35m47.    Inference\u001b[0m\n\u001b[1m\u001b[35m48.    Inference\u001b[0m\n\u001b[1m\u001b[35m49.    Inference\u001b[0m\n\u001b[1m\u001b[35m50.    Inference\u001b[0m\n\u001b[1m\u001b[35m51.    Inference\u001b[0m\n\u001b[1m\u001b[35m52.    Inference\u001b[0m\n\u001b[1m\u001b[35m53.    Inference\u001b[0m\n\u001b[1m\u001b[35m54.    Inference\u001b[0m\n\u001b[1m\u001b[35m55.    Inference\u001b[0m\n\u001b[1m\u001b[35m56.    Inference\u001b[0m\n\u001b[1m\u001b[35m57.    Inference\u001b[0m\n\u001b[1m\u001b[35m58.    Inference\u001b[0m\n\u001b[1m\u001b[35m59.    Inference\u001b[0m\n\u001b[1m\u001b[35m60.    Inference\u001b[0m\n\u001b[1m\u001b[35m61.    Inference\u001b[0m\n\u001b[1m\u001b[35m62.    Inference\u001b[0m\n\u001b[1m\u001b[35m63.    Inference\u001b[0m\n\u001b[1m\u001b[35m64.    Inference\u001b[0m\n\u001b[1m\u001b[35m65.    Inference\u001b[0m\n\u001b[1m\u001b[35m66.    Inference\u001b[0m\n\u001b[1m\u001b[35m67.    Inference\u001b[0m\n\u001b[1m\u001b[35m68.    Inference\u001b[0m\n\u001b[1m\u001b[35m69.    Inference\u001b[0m\n\u001b[1m\u001b[35m70.    Inference\u001b[0m\n\u001b[1m\u001b[35m71.    Inference\u001b[0m\n\u001b[1m\u001b[35m72.    Inference\u001b[0m\n\u001b[1m\u001b[35m73.    Inference\u001b[0m\n\u001b[1m\u001b[35m74.    Inference\u001b[0m\n\u001b[1m\u001b[35m75.    Inference\u001b[0m\n\u001b[1m\u001b[35m76.    Inference\u001b[0m\n\u001b[1m\u001b[35m77.    Inference\u001b[0m\n\u001b[1m\u001b[35m78.    Inference\u001b[0m\n\u001b[1m\u001b[35m79.    Inference\u001b[0m\n\u001b[1m\u001b[35m80.    Inference\u001b[0m\n\u001b[1m\u001b[35m81.    Inference\u001b[0m\n\u001b[1m\u001b[35m82.    Inference\u001b[0m\n\u001b[1m\u001b[35m83.    Inference\u001b[0m\n\u001b[1m\u001b[35m84.    Inference\u001b[0m\n\u001b[1m\u001b[35m85.    Inference\u001b[0m\n\u001b[1m\u001b[35m86.    Inference\u001b[0m\n\u001b[1m\u001b[35m87.    Inference\u001b[0m\n\u001b[1m\u001b[35m88.    Inference\u001b[0m\n\u001b[1m\u001b[35m89.    Inference\u001b[0m\n\u001b[1m\u001b[35m90.    Inference\u001b[0m\n\u001b[1m\u001b[35m91.    Inference\u001b[0m\n\u001b[1m\u001b[35m92.    Inference\u001b[0m\n\u001b[1m\u001b[35m93.    Inference\u001b[0m\n\u001b[1m\u001b[35m94.    Inference\u001b[0m\n\u001b[1m\u001b[35m95.    Inference\u001b[0m\n\u001b[1m\u001b[35m96.    Inference\u001b[0m\n\u001b[1m\u001b[35m97.    Inference\u001b[0m\n\u001b[1m\u001b[35m98.    Inference\u001b[0m\n\u001b[1m\u001b[35m99.    Inference\u001b[0m\n\u001b[1m\u001b[35m100.   Inference\u001b[0m\n\u001b[1m\u001b[35m101.   Inference\u001b[0m\n\u001b[1m\u001b[35m102.   Inference\u001b[0m\n\u001b[1m\u001b[35m103.   Inference\u001b[0m\n\u001b[1m\u001b[35m104.   Inference\u001b[0m\n\u001b[1m\u001b[35m105.   Inference\u001b[0m\n\u001b[1m\u001b[35m106.   Inference\u001b[0m\n\u001b[1m\u001b[35m107.   Inference\u001b[0m\n\u001b[1m\u001b[35m108.   Inference\u001b[0m\n\u001b[1m\u001b[35m109.   Inference\u001b[0m\n\u001b[1m\u001b[35m110.   Inference\u001b[0m\n\u001b[1m\u001b[35m111.   Inference\u001b[0m\n\u001b[1m\u001b[35m112.   Inference\u001b[0m\n\u001b[1m\u001b[35m113.   Inference\u001b[0m\n\u001b[1m\u001b[35m114.   Inference\u001b[0m\n\u001b[1m\u001b[35m115.   Inference\u001b[0m\n\u001b[1m\u001b[35m116.   Inference\u001b[0m\n\u001b[1m\u001b[35m117.   Inference\u001b[0m\n\u001b[1m\u001b[35m118.   Inference\u001b[0m\n\u001b[1m\u001b[35m119.   Inference\u001b[0m\n\u001b[1m\u001b[35m120.   Inference\u001b[0m\n\u001b[1m\u001b[35m121.   Inference\u001b[0m\n\u001b[1m\u001b[35m122.   Inference\u001b[0m\n\u001b[1m\u001b[35m123.   Inference\u001b[0m\n\u001b[1m\u001b[35m124.   Inference\u001b[0m\n\u001b[1m\u001b[35m125.   Inference\u001b[0m\n\u001b[1m\u001b[35m126.   Inference\u001b[0m\n\u001b[1m\u001b[35m127.   Inference\u001b[0m\n\u001b[1m\u001b[35m128.   Inference\u001b[0m\n\u001b[1m\u001b[35m129.   Inference\u001b[0m\n\u001b[1m\u001b[35m130.   Inference\u001b[0m\n\u001b[1m\u001b[35m131.   Inference\u001b[0m\n\u001b[1m\u001b[35m132.   Inference\u001b[0m\n\u001b[1m\u001b[35m133.   Inference\u001b[0m\n\u001b[1m\u001b[35m134.   Inference\u001b[0m\n\u001b[1m\u001b[35m135.   Inference\u001b[0m\n\u001b[1m\u001b[35m136.   Inference\u001b[0m\n\u001b[1m\u001b[35m137.   Inference\u001b[0m\n\u001b[1m\u001b[35m138.   Inference\u001b[0m\n\u001b[1m\u001b[35m139.   Inference\u001b[0m\n\u001b[1m\u001b[35m140.   Inference\u001b[0m\n\u001b[1m\u001b[35m141.   Inference\u001b[0m\n\u001b[1m\u001b[35m142.   Inference\u001b[0m\n\u001b[1m\u001b[35m143.   Inference\u001b[0m\n\u001b[1m\u001b[35m144.   Inference\u001b[0m\n\u001b[1m\u001b[35m145.   Inference\u001b[0m\n\u001b[1m\u001b[35m146.   Inference\u001b[0m\n\u001b[1m\u001b[35m147.   Inference\u001b[0m\n\u001b[1m\u001b[35m148.   Inference\u001b[0m\n\u001b[1m\u001b[35m149.   Inference\u001b[0m\n\u001b[1m\u001b[35m150.   Inference\u001b[0m\n\u001b[1m\u001b[35m151.   Inference\u001b[0m\n\u001b[1m\u001b[35m152.   Inference\u001b[0m\n\u001b[1m\u001b[35m153.   Inference\u001b[0m\n\u001b[1m\u001b[35m154.   Inference\u001b[0m\n\u001b[1m\u001b[35m155.   Inference\u001b[0m\n\u001b[1m\u001b[35m156.   Inference\u001b[0m\n\u001b[1m\u001b[35m157.   Inference\u001b[0m\n\u001b[1m\u001b[35m158.   Inference\u001b[0m\n\u001b[1m\u001b[35m159.   Inference\u001b[0m\n\u001b[1m\u001b[35m160.   Inference\u001b[0m\n\u001b[1m\u001b[35m161.   Inference\u001b[0m\n\u001b[1m\u001b[35m162.   Inference\u001b[0m\n\u001b[1m\u001b[35m163.   Inference\u001b[0m\n\u001b[1m\u001b[35m164.   Inference\u001b[0m\n\u001b[1m\u001b[35m165.   Inference\u001b[0m\n\u001b[1m\u001b[34m\n---> Submission file\n\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      row_id    target\n0  480_540_0 -1.273597\n1  480_540_1 -0.520258\n2  480_540_2  0.603207\n3  480_540_3 -2.060603\n4  480_540_4 -1.232059\n5  480_540_5  2.931046\n6  480_540_6  1.318050\n7  480_540_7 -1.431486\n8  480_540_8  0.599811\n9  480_540_9 -0.419547","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>480_540_0</td>\n      <td>-1.273597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>480_540_1</td>\n      <td>-0.520258</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>480_540_2</td>\n      <td>0.603207</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>480_540_3</td>\n      <td>-2.060603</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>480_540_4</td>\n      <td>-1.232059</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>480_540_5</td>\n      <td>2.931046</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>480_540_6</td>\n      <td>1.318050</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>480_540_7</td>\n      <td>-1.431486</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>480_540_8</td>\n      <td>0.599811</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>480_540_9</td>\n      <td>-0.419547</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n\u001b[1m\u001b[31m\nRAM memory GB usage = 0.5078\u001b[0m\nCPU times: user 1min 11s, sys: 938 ms, total: 1min 12s\nWall time: 1min 3s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# <div style= \"font-family: Cambria; font-weight:bold; letter-spacing: 0px; color:#ffffff; font-size:120%; text-align:left;padding:3.0px; background: #003380; border-bottom: 10px solid #80ffff\"> OUTRO<br><div> ","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style = \"font-family: Cambria Math;font-size: 115%; color: black; background-color: #e6f9ff; border: dashed black 1.0px; padding: 3.5px\" >\n<b>Next steps</b> <br>\n1. Exploring better models and ensemble strategy <br>\n2. Purging redundant features from the existing list of features <br>\n3. Fostering improvements in the existing process based on public discussions and kernels<br>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<b>References</b> <br>\n1. https://www.kaggle.com/code/yuanzhezhou/baseline-lgb-xgb-and-catboost <br>\n2. https://www.kaggle.com/code/renatoreggiani/optv-lightgbm -- Median volume column <br> \n3. https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models -- goto conversion <br>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" align = \"center\" style = \"font-family: Calibri;font-size: 150%; color: black; background-color:#ccf2ff; border: solid black 2.5px; padding: 3.5px\" >\n    <b>If you find this useful, please upvote the kernel and the input kernel and dataset too. <br> Best regards!</b>\n</div>","metadata":{}}]}