{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9fc513",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-26T16:00:18.371233Z",
     "iopub.status.busy": "2024-03-26T16:00:18.370801Z",
     "iopub.status.idle": "2024-03-26T16:00:23.620189Z",
     "shell.execute_reply": "2024-03-26T16:00:23.619223Z"
    },
    "papermill": {
     "duration": 5.260688,
     "end_time": "2024-03-26T16:00:23.622470",
     "exception": false,
     "start_time": "2024-03-26T16:00:18.361782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.13\n"
     ]
    }
   ],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# ðŸ“¦ Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ðŸ¤ Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "import time\n",
    "import polars as pl\n",
    "import gc\n",
    "print(pl.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964a2bfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:00:23.637097Z",
     "iopub.status.busy": "2024-03-26T16:00:23.636801Z",
     "iopub.status.idle": "2024-03-26T16:00:23.990318Z",
     "shell.execute_reply": "2024-03-26T16:00:23.989565Z"
    },
    "papermill": {
     "duration": 0.363188,
     "end_time": "2024-03-26T16:00:23.992565",
     "exception": false,
     "start_time": "2024-03-26T16:00:23.629377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import catboost as cbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe59611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:00:24.006795Z",
     "iopub.status.busy": "2024-03-26T16:00:24.006500Z",
     "iopub.status.idle": "2024-03-26T16:00:24.014742Z",
     "shell.execute_reply": "2024-03-26T16:00:24.013831Z"
    },
    "papermill": {
     "duration": 0.017455,
     "end_time": "2024-03-26T16:00:24.016718",
     "exception": false,
     "start_time": "2024-03-26T16:00:23.999263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/kaggle/input/optiver/xgb3_feas_v7_157.json\") as f:\n",
    "    feas_dict = json.load(f)\n",
    "selected_feas = feas_dict['selected_feas']\n",
    "print(len(selected_feas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b66306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:00:24.030389Z",
     "iopub.status.busy": "2024-03-26T16:00:24.030116Z",
     "iopub.status.idle": "2024-03-26T16:00:24.034082Z",
     "shell.execute_reply": "2024-03-26T16:00:24.033338Z"
    },
    "papermill": {
     "duration": 0.01295,
     "end_time": "2024-03-26T16:00:24.035931",
     "exception": false,
     "start_time": "2024-03-26T16:00:24.022981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53eedb0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:00:24.050016Z",
     "iopub.status.busy": "2024-03-26T16:00:24.049274Z",
     "iopub.status.idle": "2024-03-26T16:01:12.528767Z",
     "shell.execute_reply": "2024-03-26T16:01:12.527991Z"
    },
    "papermill": {
     "duration": 48.488974,
     "end_time": "2024-03-26T16:01:12.531202",
     "exception": false,
     "start_time": "2024-03-26T16:00:24.042228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_model_list = []\n",
    "for k in [1,2,3,4,5]:\n",
    "    model=xgb.XGBRegressor()\n",
    "    model.load_model(f'/kaggle/input/optiver/xgb3_v7_k{k}_weight15_debug.json') \n",
    "    xgb_model_list.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e7f386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.546269Z",
     "iopub.status.busy": "2024-03-26T16:01:12.545955Z",
     "iopub.status.idle": "2024-03-26T16:01:12.564734Z",
     "shell.execute_reply": "2024-03-26T16:01:12.563892Z"
    },
    "papermill": {
     "duration": 0.02864,
     "end_time": "2024-03-26T16:01:12.566813",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.538173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None,\n",
       "              feature_types=['float', 'float', 'float', 'float', 'float', 'int',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'flo...\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None,\n",
       "              feature_types=['float', 'float', 'float', 'float', 'float', 'int',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'flo...\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None,\n",
       "              feature_types=['float', 'float', 'float', 'float', 'float', 'int',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'flo...\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None,\n",
       "              feature_types=['float', 'float', 'float', 'float', 'float', 'int',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'flo...\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...),\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None,\n",
       "              feature_types=['float', 'float', 'float', 'float', 'float', 'int',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'float', 'float', 'float', 'float', 'float',\n",
       "                             'flo...\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f18c46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.581720Z",
     "iopub.status.busy": "2024-03-26T16:01:12.581421Z",
     "iopub.status.idle": "2024-03-26T16:01:12.589646Z",
     "shell.execute_reply": "2024-03-26T16:01:12.588939Z"
    },
    "papermill": {
     "duration": 0.017725,
     "end_time": "2024-03-26T16:01:12.591605",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.573880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/optiver/scale_dict_median.json\") as f:\n",
    "    scale_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10775251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.607300Z",
     "iopub.status.busy": "2024-03-26T16:01:12.607017Z",
     "iopub.status.idle": "2024-03-26T16:01:12.671344Z",
     "shell.execute_reply": "2024-03-26T16:01:12.670479Z"
    },
    "papermill": {
     "duration": 0.074478,
     "end_time": "2024-03-26T16:01:12.673220",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.598742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_features_no_hist_polars(df, target_feas):\n",
    "    size_col = ['imbalance_size','matched_size','bid_size','ask_size']\n",
    "    for _ in size_col:\n",
    "        tmp_map = scale_dict[_].copy()\n",
    "        tmp_map = {int(k):v for k,v in tmp_map.items()}\n",
    "        df[f\"{_}_stock_median\"] = df['stock_id'].map(tmp_map)\n",
    "        df[f\"scale_{_}\"] = df[_] / df[f\"{_}_stock_median\"]\n",
    "        del df[f\"{_}_stock_median\"]\n",
    "    #buy-side imbalance; 1\n",
    "    #sell-side imbalance; -1\n",
    "    #no imbalance; 0\n",
    "    df['auc_bid_size'] = df['matched_size']\n",
    "    df['auc_ask_size'] = df['matched_size']\n",
    "    df.loc[df['imbalance_buy_sell_flag']==1,'auc_bid_size'] += df.loc[df['imbalance_buy_sell_flag']==1,'imbalance_size']\n",
    "    df.loc[df['imbalance_buy_sell_flag']==-1,'auc_ask_size'] += df.loc[df['imbalance_buy_sell_flag']==-1,'imbalance_size']\n",
    "    # åŠ ä¸€ä¸ªask_size - bid_sizeçš„ç‰¹å¾ ç„¶åŽRolling\n",
    "    df = pl.from_pandas(df)\n",
    "    feas_list = ['stock_id','seconds_in_bucket','imbalance_size','imbalance_buy_sell_flag',\n",
    "               'reference_price','matched_size','far_price','near_price','bid_price','bid_size',\n",
    "                'ask_price','ask_size','wap','scale_imbalance_size','scale_matched_size','scale_bid_size','scale_ask_size'\n",
    "                 ,'auc_bid_size','auc_ask_size']\n",
    "    # åŸºç¡€ç‰¹å¾\n",
    "    df = df.with_columns([\n",
    "        # é˜¶æ®µ1\n",
    "        (pl.col('ask_size') * pl.col('ask_price')).alias(\"ask_money\"),\n",
    "        (pl.col('bid_size') * pl.col('bid_price')).alias(\"bid_money\"),\n",
    "        (pl.col('ask_size') + pl.col(\"auc_ask_size\")).alias(\"ask_size_all\"),\n",
    "        (pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"bid_size_all\"),\n",
    "        (pl.col('ask_size') + pl.col(\"auc_ask_size\") + pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"volumn_size_all\"),\n",
    "        (pl.col('reference_price') * pl.col('auc_ask_size')).alias(\"ask_auc_money\"),\n",
    "        (pl.col('reference_price') * pl.col('auc_bid_size')).alias(\"bid_auc_money\"),\n",
    "        (pl.col('ask_size') * pl.col('ask_price') + pl.col('bid_size') * pl.col('bid_price')).alias(\"volumn_money\"),\n",
    "        (pl.col('ask_size') + pl.col('bid_size')).alias('volume_cont'),\n",
    "        (pl.col('ask_size') - pl.col('bid_size')).alias('diff_ask_bid_size'),\n",
    "        (pl.col('imbalance_size') + 2 * pl.col('matched_size')).alias('volumn_auc'),\n",
    "        ((pl.col('imbalance_size') + 2 * pl.col('matched_size')) * pl.col(\"reference_price\")).alias('volumn_auc_money'),\n",
    "        ((pl.col('ask_price') + pl.col('bid_price'))/2).alias('mid_price'),\n",
    "        ((pl.col('near_price') + pl.col('far_price'))/2).alias('mid_price_near_far'),\n",
    "        (pl.col('ask_price') - pl.col('bid_price')).alias('price_diff_ask_bid'),\n",
    "        (pl.col('ask_price') / pl.col('bid_price')).alias('price_div_ask_bid'),\n",
    "        (pl.col('imbalance_buy_sell_flag') * pl.col('scale_imbalance_size')).alias('flag_scale_imbalance_size'),\n",
    "        (pl.col('imbalance_buy_sell_flag') * pl.col('imbalance_size')).alias('flag_imbalance_size'),\n",
    "        (pl.col('imbalance_size') / pl.col('matched_size') * pl.col('imbalance_buy_sell_flag')).alias(\"div_flag_imbalance_size_2_balance\"),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size')).alias('price_pressure'),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size') * pl.col('imbalance_buy_sell_flag')).alias('price_pressure_v2'),\n",
    "        ((pl.col(\"ask_size\") - pl.col(\"bid_size\")) / (pl.col(\"far_price\") - pl.col(\"near_price\"))).alias(\"depth_pressure\"),\n",
    "        (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"div_bid_size_ask_size\"),\n",
    "    ])\n",
    "    feas_list.extend(['ask_money', 'bid_money', 'ask_auc_money','bid_auc_money',\"ask_size_all\",\"bid_size_all\",\"volumn_size_all\",\n",
    "                      'volumn_money','volume_cont',\"volumn_auc\",\"volumn_auc_money\",\"mid_price\",\n",
    "                      'mid_price_near_far','price_diff_ask_bid',\"price_div_ask_bid\",\"flag_imbalance_size\",\"div_flag_imbalance_size_2_balance\",\n",
    "                     \"price_pressure\",\"price_pressure_v2\",\"depth_pressure\",\"flag_scale_imbalance_size\",\"diff_ask_bid_size\"])        \n",
    "\n",
    "    # å„ç§ratio\n",
    "    # æå‡å¾®å¿½å‡ å¾®\n",
    "    add_cols = []\n",
    "    for col1, col2 in [\n",
    "        (\"imbalance_size\",\"bid_size\"),\n",
    "        (\"imbalance_size\",\"ask_size\"),\n",
    "        (\"matched_size\",\"bid_size\"),\n",
    "        (\"matched_size\",\"ask_size\"),\n",
    "        (\"imbalance_size\",\"volume_cont\"),\n",
    "        (\"matched_size\",\"volume_cont\"),\n",
    "        (\"auc_bid_size\",\"bid_size\"),\n",
    "        (\"auc_ask_size\",\"ask_size\"),\n",
    "        (\"bid_auc_money\",\"bid_money\"),\n",
    "        (\"ask_auc_money\",\"ask_money\"),\n",
    "    ]:\n",
    "        add_cols.append((pl.col(col1) / pl.col(col2)).alias(f\"div_{col1}_2_{col2}\"))\n",
    "        feas_list.append(f\"div_{col1}_2_{col2}\")        \n",
    "    df = df.with_columns(add_cols)\n",
    "\n",
    "    # é˜¶æ®µ2 ä¸å¹³è¡¡ç‰¹å¾\n",
    "    # é™¤äº†priceç›¸å…³\n",
    "    # æ²¡åŠ aucçš„ask/bidçš„ æž„é€ priceä»¥åŠä¸å¹³è¡¡è¿›åŽ»\n",
    "    add_cols = []\n",
    "    for pair1,pair2 in [\n",
    "        ('ask_size','bid_size'),\n",
    "        ('ask_money','bid_money'),\n",
    "        ('volumn_money','volumn_auc_money'),\n",
    "        ('volume_cont','volumn_auc'),\n",
    "        ('imbalance_size','matched_size'),\n",
    "        ('auc_ask_size','auc_bid_size'),\n",
    "        (\"ask_size_all\",'bid_size_all')\n",
    "    ]:\n",
    "        col_imb = f\"imb1_{pair1}_{pair2}\"\n",
    "        add_cols.extend([\n",
    "            ((pl.col(pair1) - pl.col(pair2)) / (pl.col(pair1) + pl.col(pair2))).alias(col_imb),\n",
    "        ])\n",
    "        feas_list.extend([col_imb])\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    # priceä¾§çš„imb1\n",
    "    fea_append_list = []\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\",\"mid_price\"]\n",
    "    for c in combinations(prices, 2):\n",
    "        fea_append_list.append(((pl.col(c[0]) - pl.col(c[1])) / (pl.col(c[0]) + pl.col(c[1]))).alias(f\"imb1_{c[0]}_{c[1]}\"))\n",
    "        # fea_append_list.append((pl.col(c[0]) - pl.col(c[1])).alias(f\"diff_{c[0]}_{c[1]}\"))\n",
    "        feas_list.extend([f\"imb1_{c[0]}_{c[1]}\"])\n",
    "    df = df.with_columns(fea_append_list)\n",
    "    \n",
    "    \n",
    "    # ä¸å¹³è¡¡ç‰¹å¾ ç´¯è®¡ä¹˜\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(\"imb1_ask_size_bid_size\") + 2) * (pl.col(\"imb1_ask_price_bid_price\") + 2) * (pl.col(\"imb1_auc_ask_size_auc_bid_size\")+2)).alias(\"market_urgency_v2\"),\n",
    "        (pl.col('price_diff_ask_bid') * (pl.col('imb1_ask_size_bid_size'))).alias('market_urgency'),\n",
    "        (pl.col('imb1_ask_price_bid_price') * (pl.col('imb1_ask_size_bid_size'))).alias('market_urgency_v3'),\n",
    "    ])\n",
    "    feas_list.extend([f\"market_urgency_v3\",'market_urgency','market_urgency_v2'])\n",
    "    \n",
    "    feas_list = ['imb1_wap_mid_price', 'imb1_ask_money_bid_money', 'imb1_volume_cont_volumn_auc', 'imb1_reference_price_ask_price', \n",
    "                 'imb1_reference_price_mid_price', 'seconds_in_bucket', 'div_flag_imbalance_size_2_balance', 'ask_price', \n",
    "                 'imb1_reference_price_bid_price', 'scale_matched_size', 'imb1_near_price_wap', 'volumn_auc_money', 'imb1_far_price_wap', \n",
    "                 'bid_size', 'scale_bid_size', 'bid_size_all']\n",
    "    # éš”ç¦»\n",
    "    add_cols = []\n",
    "    for col in [\"bid_auc_money\",\"imb1_reference_price_wap\",\"bid_size_all\",\n",
    "                \"imb1_auc_ask_size_auc_bid_size\",\"div_flag_imbalance_size_2_balance\",\n",
    "                \"imb1_ask_size_all_bid_size_all\",\"flag_imbalance_size\",\"imb1_reference_price_mid_price\"]:\n",
    "        for window in [3,6,18,36,60]:\n",
    "            add_cols.append(pl.col(col).rolling_mean(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_mean_{col}'))\n",
    "            add_cols.append(pl.col(col).rolling_std(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_std_{col}'))\n",
    "            feas_list.extend([f'rolling{window}_mean_{col}',f'rolling{window}_std_{col}'])\n",
    "    feas_list = ['imb1_wap_mid_price', 'imb1_ask_money_bid_money', 'imb1_volume_cont_volumn_auc', \n",
    "                     'imb1_reference_price_ask_price', 'imb1_reference_price_mid_price', \n",
    "                     'seconds_in_bucket', 'div_flag_imbalance_size_2_balance', 'ask_price', \n",
    "                     'imb1_reference_price_bid_price', 'scale_matched_size', 'imb1_near_price_wap', \n",
    "                     'volumn_auc_money', 'imb1_far_price_wap', 'bid_size', 'scale_bid_size', 'bid_size_all', \n",
    "                     'rolling18_mean_imb1_auc_ask_size_auc_bid_size', 'rolling3_mean_div_flag_imbalance_size_2_balance', \n",
    "                     'rolling60_std_div_flag_imbalance_size_2_balance', 'rolling36_mean_flag_imbalance_size', \n",
    "                     'rolling3_std_imb1_auc_ask_size_auc_bid_size', 'rolling18_mean_imb1_ask_size_all_bid_size_all', \n",
    "                     'rolling6_mean_div_flag_imbalance_size_2_balance', 'rolling6_std_imb1_auc_ask_size_auc_bid_size', \n",
    "                     'rolling3_mean_imb1_auc_ask_size_auc_bid_size', 'rolling60_std_imb1_auc_ask_size_auc_bid_size', \n",
    "                     'rolling6_std_bid_size_all', 'rolling3_std_bid_size_all', 'rolling3_mean_bid_size_all', \n",
    "                     'rolling18_std_bid_auc_money', 'rolling36_mean_bid_auc_money',\"rolling60_mean_imb1_reference_price_wap\",\n",
    "                    'rolling18_mean_imb1_reference_price_wap', 'rolling3_mean_imb1_reference_price_mid_price']\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "#     for col in [\"flag_imbalance_size\",\"imb1_reference_price_wap\",\"imb1_reference_price_mid_price\",\"mid_price\",\"imb1_far_price_wap\",\n",
    "#                'matched_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "#         add_cols = []\n",
    "#         for window_size in [1,2,4,6,12]:\n",
    "#             add_cols.append(pl.col(col).shift(window_size).over('stock_id','date_id').alias(f'shift{window_size}_{col}'))\n",
    "#             add_cols.append((pl.col(col) / pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'div_shift{window_size}_{col}'))\n",
    "#             add_cols.append((pl.col(col) - pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'diff_shift{window_size}_{col}'))\n",
    "#             feas_list.extend([f'shift{window_size}_{col}',f'div_shift{window_size}_{col}',f'diff_shift{window_size}_{col}'])\n",
    "#         df = df.with_columns(add_cols)\n",
    "    ### æ‚ä¸ƒæ‚å…«\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"flag_imbalance_size\").diff().over('stock_id','date_id').alias(\"imbalance_momentum_unscaled\"),\n",
    "        pl.col(\"price_diff_ask_bid\").diff().over('stock_id','date_id').alias(\"spread_intensity\"),\n",
    "    ])\n",
    "    feas_list.extend([\"imbalance_momentum_unscaled\",\"spread_intensity\"])\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"imbalance_momentum_unscaled\")/pl.col(\"matched_size\")).alias(\"imbalance_momentum\")\n",
    "    ])\n",
    "    feas_list.extend([\"imbalance_momentum\"])\n",
    "\n",
    "    #Calculate diff features for specific columns\n",
    "    add_cols = []\n",
    "    for col in ['ask_price',\n",
    " 'bid_price',\n",
    " 'imb1_reference_price_near_price',\n",
    " 'bid_size',\n",
    " 'scale_bid_size',\n",
    " 'mid_price',\n",
    " 'ask_size',\n",
    " 'price_div_ask_bid',\n",
    " 'div_bid_size_ask_size',\n",
    " 'market_urgency',\n",
    " 'wap',\n",
    " 'imbalance_momentum']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            add_cols.append((pl.col(col).diff(window).over('stock_id','date_id')).alias(f\"{col}_diff_{window}\"))\n",
    "            feas_list.append(f\"{col}_diff_{window}\")\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    ### target mockç³»åˆ—\n",
    "    for mock_period in [1,3,12,6]:\n",
    "    \n",
    "        df = df.with_columns([\n",
    "            pl.col(\"wap\").shift(-mock_period).over(\"stock_id\",\"date_id\").alias(f\"wap_shift_n{mock_period}\")\n",
    "        ])\n",
    "        df = df.with_columns([\n",
    "            (pl.col(f\"wap_shift_n{mock_period}\")/pl.col(\"wap\")).alias(\"target_single\")\n",
    "        ])\n",
    "\n",
    "        tmp_df = df.select(pl.col(\"target_single\"),pl.col(\"weight\")).to_pandas()\n",
    "        tmp_df.loc[tmp_df[\"target_single\"].isna(),\"weight\"] = 0\n",
    "        df = df.with_columns([\n",
    "            pl.lit(np.array(tmp_df[\"weight\"])).alias(\"weight_tmp\")\n",
    "        ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            (((pl.col(\"weight_tmp\") * pl.col(\"target_single\")).sum().over(\"date_id\",\"seconds_in_bucket\")) / ((pl.col(\"weight_tmp\")).sum().over(\"date_id\",\"seconds_in_bucket\"))).alias(\"index_target_mock\")\n",
    "        ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            ((pl.col(\"target_single\") - pl.col(\"index_target_mock\"))*10000).alias(\"target_mock\")\n",
    "        ])\n",
    "\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"target_mock\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"target_mock_shift{mock_period}\"),\n",
    "            #pl.col(\"index_target_mock\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"index_target_mock_shift{mock_period}\"),\n",
    "            #pl.col(\"target_single\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"target_single_shift{mock_period}\")\n",
    "        ])\n",
    "    # df.drop_in_place(\"wap_shift_6\")\n",
    "    # df.drop_in_place(\"target_single_shift6\")\n",
    "    # df.drop_in_place(\"indexwap_shift6\")\n",
    "    # add_cols_new = []\n",
    "    add_cols = []\n",
    "    for col in ['target_mock_shift6','target_mock_shift1','target_mock_shift3','target_mock_shift12']:\n",
    "        for window in [1, 3,6,12,24,48]:\n",
    "            add_cols.append(pl.col(col).rolling_mean(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_mean_{col}'))\n",
    "            #add_cols.append(pl.col(col).rolling_std(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_std_{col}'))\n",
    "            # add_cols_new.extend([f'rolling{window}_mean_{col}'])\n",
    "    df = df.with_columns(add_cols)\n",
    "    keep_cols_new = ['rolling48_mean_target_mock_shift3', 'rolling48_mean_target_mock_shift1', 'rolling48_mean_target_mock_shift12',\n",
    "'rolling1_mean_target_mock_shift6', 'rolling24_mean_target_mock_shift6','rolling24_mean_target_mock_shift12',]\n",
    "    feas_list.extend(keep_cols_new)\n",
    "    \n",
    "    add_cols = []\n",
    "    for col in [\"imb1_auc_ask_size_auc_bid_size\",\"flag_imbalance_size\",\"price_pressure_v2\",\"scale_matched_size\"]:\n",
    "        for window_size in [1,2,3,6,12]:\n",
    "            add_cols.append(pl.col(col).shift(window_size).over('stock_id','date_id').alias(f'shift{window_size}_{col}'))\n",
    "            add_cols.append((pl.col(col) / pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'div_shift{window_size}_{col}'))\n",
    "            add_cols.append((pl.col(col) - pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'diff_shift{window_size}_{col}'))\n",
    "            #feas_list.extend([f'shift{window_size}_{col}',f'div_shift{window_size}_{col}',f'diff_shift{window_size}_{col}'])\n",
    "    feas_list.extend(['div_shift6_imb1_auc_ask_size_auc_bid_size',\n",
    " 'diff_shift6_price_pressure_v2',\n",
    " 'shift1_price_pressure_v2',\n",
    " 'div_shift3_flag_imbalance_size',\n",
    " 'div_shift12_imb1_auc_ask_size_auc_bid_size',\n",
    " 'div_shift3_scale_matched_size',\n",
    " 'diff_shift6_flag_imbalance_size',\n",
    " 'shift12_imb1_auc_ask_size_auc_bid_size',\n",
    " 'div_shift12_price_pressure_v2',\n",
    " 'shift6_flag_imbalance_size',\n",
    " 'diff_shift3_imb1_auc_ask_size_auc_bid_size',\n",
    " 'div_shift12_flag_imbalance_size',\n",
    " 'shift12_flag_imbalance_size'])\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    add_cols = []\n",
    "    for col in ['imb1_ask_price_mid_price',\n",
    " 'market_urgency',\n",
    " 'market_urgency_diff_1',\n",
    " 'imb1_ask_money_bid_money',\n",
    " 'rolling18_mean_imb1_ask_size_all_bid_size_all',\n",
    " 'rolling18_mean_imb1_auc_ask_size_auc_bid_size',\n",
    " 'rolling18_mean_imb1_reference_price_wap',\n",
    " 'ask_price_diff_3',\n",
    " 'diff_shift1_price_pressure_v2',\n",
    " 'diff_shift12_scale_matched_size',\n",
    " 'diff_shift1_flag_imbalance_size',\n",
    " 'imb1_ask_size_bid_size',\n",
    " 'imb1_bid_price_mid_price',\n",
    " 'rolling48_mean_target_mock_shift6']:\n",
    "        add_cols.append((((pl.col(col) * pl.col(\"weight\")).sum().over(\"date_id\",\"seconds_in_bucket\"))/(((pl.col(\"weight\")).sum().over(\"date_id\",\"seconds_in_bucket\")))).alias(f\"global_{col}\"))\n",
    "        feas_list.append(f\"global_{col}\")\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    \n",
    "    # MACD\n",
    "    rsi_cols = [\"mid_price_near_far\",\"imb1_reference_price_wap\",\"near_price\",]\n",
    "    add_cols = []\n",
    "    for col in rsi_cols:\n",
    "        for window_size in [3,6,12,24,48]:\n",
    "            add_cols.append(pl.col(col).ewm_mean(span=window_size, adjust=False).over('stock_id','date_id').alias(f\"rolling_ewm_{window_size}_{col}\"))\n",
    "            #feas_list.append(f\"rolling_ewm_{window_size}_{col}\")\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    add_cols = []\n",
    "    for col in rsi_cols:\n",
    "        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n",
    "            add_cols.append((pl.col(f\"rolling_ewm_{w1}_{col}\") - pl.col(f\"rolling_ewm_{w2}_{col}\")).alias(f\"dif_{col}_{w1}_{w2}\"))\n",
    "            #feas_list.append(f\"dif_{col}_{w1}_{w2}\")\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    add_cols = []\n",
    "    for col in rsi_cols:\n",
    "        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n",
    "            add_cols.append(pl.col(f\"dif_{col}_{w1}_{w2}\").ewm_mean(span=9, adjust=False).over('stock_id','date_id').alias(f\"dea_{col}_{w1}_{w2}\"))\n",
    "            #feas_list.append(f\"dea_{col}_{w1}_{w2}\")\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    add_cols = []\n",
    "    for col in rsi_cols:\n",
    "        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n",
    "            add_cols.append((pl.col(f\"dif_{col}_{w1}_{w2}\") - pl.col(f\"dea_{col}_{w1}_{w2}\")).alias(f\"macd_{col}_{w1}_{w2}\"))\n",
    "            #feas_list.append(f\"macd_{col}_{w1}_{w2}\")\n",
    "    \n",
    "    feas_list.extend(['macd_imb1_reference_price_wap_12_24',\n",
    " 'dif_imb1_reference_price_wap_3_6',\n",
    " 'macd_mid_price_near_far_12_24',\n",
    " 'dif_near_price_3_6',\n",
    " 'macd_near_price_24_48',\n",
    " 'dea_imb1_reference_price_wap_12_24',\n",
    " 'macd_near_price_12_24',\n",
    " 'rolling_ewm_24_imb1_reference_price_wap',\n",
    " 'dif_near_price_6_12',\n",
    " 'dea_mid_price_near_far_6_12',\n",
    " 'dea_near_price_24_48',\n",
    " 'rolling_ewm_12_imb1_reference_price_wap',\n",
    " 'dif_imb1_reference_price_wap_12_24'])\n",
    "    df = df.with_columns(add_cols)\n",
    "    \n",
    "    #add_cols = []\n",
    "    new_add_cols = []\n",
    "    for col in [\"target\"]:\n",
    "        # 176 1,2,3,5,10,15,20,25,30\n",
    "        # [1,2,3,5,10,15,20,25,30,35,40,45,60] 5.8704926 157\n",
    "        # [1,2,3,5,10,15,20,30,45,60] 5.8708683137\n",
    "        for window_size in [1,2,3,5,10,15,20,25,30,35,40,45,60]:\n",
    "            #add_cols.append(pl.col(col).shift(1).rolling_mean(window_size=window_size,min_periods=1).over('stock_id','seconds_in_bucket').alias(f'rolling_mean_{window_size}_{col}_second'))\n",
    "            #add_cols.append(pl.col(col).shift(1).rolling_std(window_size=window_size,min_periods=1).over('stock_id','seconds_in_bucket').alias(f'rolling_std_{window_size}_{col}_second'))\n",
    "\n",
    "            \n",
    "            feas_list.extend([f'rolling_mean_{window_size}_{col}_second',f'rolling_std_{window_size}_{col}_second',])\n",
    "            new_add_cols.extend([f'rolling_mean_{window_size}_{col}_second',f'rolling_std_{window_size}_{col}_second',])\n",
    "    #df = df.with_columns(add_cols)\n",
    "    \n",
    "    df = df.join(target_feas,how='left',on=['stock_id','date_id','seconds_in_bucket'])\n",
    "    keep_cols = ['stock_id','date_id']\n",
    "    keep_all = keep_cols + feas_list \n",
    "    return df.to_pandas()[keep_all], feas_list\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0422590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.687862Z",
     "iopub.status.busy": "2024-03-26T16:01:12.687547Z",
     "iopub.status.idle": "2024-03-26T16:01:12.699832Z",
     "shell.execute_reply": "2024-03-26T16:01:12.699000Z"
    },
    "papermill": {
     "duration": 0.021887,
     "end_time": "2024-03-26T16:01:12.701767",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.679880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3573549e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.716178Z",
     "iopub.status.busy": "2024-03-26T16:01:12.715915Z",
     "iopub.status.idle": "2024-03-26T16:01:12.732059Z",
     "shell.execute_reply": "2024-03-26T16:01:12.731182Z"
    },
    "papermill": {
     "duration": 0.025673,
     "end_time": "2024-03-26T16:01:12.734060",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.708387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_format(test_df,revealed_targets):\n",
    "    \n",
    "    \n",
    "    target_df = pd.DataFrame()\n",
    "    if len(revealed_targets) > 2:\n",
    "        # å¢žåŠ try except\n",
    "        try:\n",
    "            target_df['stock_id'] = list(revealed_targets['stock_id'])\n",
    "            target_df['date_id'] = list(revealed_targets['revealed_date_id'])\n",
    "            target_df['seconds_in_bucket'] = list(revealed_targets['seconds_in_bucket'])\n",
    "            target_df['target'] = list(revealed_targets['revealed_target'])\n",
    "            target_df['stock_id'] = target_df['stock_id'].astype(np.int64)\n",
    "            target_df['date_id'] = target_df['date_id'].astype(np.int64)\n",
    "            target_df['seconds_in_bucket'] = target_df['seconds_in_bucket'].astype(np.int64)\n",
    "            target_df['target'] = target_df['target'].astype(np.float64)\n",
    "        except:\n",
    "            target_df = pd.DataFrame()\n",
    "    \n",
    "    # test_df\n",
    "    test_df['stock_id'] = test_df['stock_id'].astype(np.int64)\n",
    "    test_df['date_id'] = test_df['date_id'].astype(np.int64)\n",
    "    test_df['seconds_in_bucket'] = test_df['seconds_in_bucket'].astype(np.int64)\n",
    "    test_df['imbalance_size'] = test_df['imbalance_size'].astype(np.float64)\n",
    "    test_df['imbalance_buy_sell_flag'] = test_df['imbalance_buy_sell_flag'].astype(np.int64)\n",
    "    test_df['reference_price'] = test_df['reference_price'].astype(np.float64)\n",
    "    test_df['matched_size'] = test_df['matched_size'].astype(np.float64)\n",
    "    test_df['far_price'] = test_df['far_price'].astype(np.float64)\n",
    "    test_df['near_price'] = test_df['near_price'].astype(np.float64)\n",
    "    test_df['bid_price'] = test_df['bid_price'].astype(np.float64)\n",
    "    test_df['bid_size'] = test_df['bid_size'].astype(np.float64)\n",
    "    test_df['ask_price'] = test_df['ask_price'].astype(np.float64)\n",
    "    test_df['ask_size'] = test_df['ask_size'].astype(np.float64)\n",
    "    test_df['wap'] = test_df['wap'].astype(np.float64)\n",
    "    return test_df, target_df\n",
    "\n",
    "def get_target_feathers(df, date_id, second, test_df_mock):\n",
    "    df = df.copy()\n",
    "    df = df[df['seconds_in_bucket']==second]\n",
    "    df = df[df['date_id']<date_id] # ä»¥é˜²é‡å¤æ•°æ®\n",
    "    df = pd.concat([df,test_df_mock],ignore_index=True).sort_values(['date_id','seconds_in_bucket','stock_id'])\n",
    "    df = pl.from_pandas(df)\n",
    "    feas_list = []\n",
    "    add_cols = []\n",
    "    for col in [\"target\"]:\n",
    "\n",
    "        for window_size in [1,2,3,5,10,15,20,25,30,35,40,45,60]:\n",
    "            add_cols.append(pl.col(col).shift(1).rolling_mean(window_size=window_size,min_periods=1).over('stock_id','seconds_in_bucket').alias(f'rolling_mean_{window_size}_{col}_second'))\n",
    "            add_cols.append(pl.col(col).shift(1).rolling_std(window_size=window_size,min_periods=1).over('stock_id','seconds_in_bucket').alias(f'rolling_std_{window_size}_{col}_second'))\n",
    "\n",
    "            \n",
    "            feas_list.extend([f'rolling_mean_{window_size}_{col}_second',f'rolling_std_{window_size}_{col}_second',])\n",
    "\n",
    "    df = df.with_columns(add_cols)\n",
    "    df = df.filter(pl.col(\"date_id\")==date_id)\n",
    "    df.drop_in_place('target')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fe13be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.748918Z",
     "iopub.status.busy": "2024-03-26T16:01:12.748676Z",
     "iopub.status.idle": "2024-03-26T16:01:12.761133Z",
     "shell.execute_reply": "2024-03-26T16:01:12.760372Z"
    },
    "papermill": {
     "duration": 0.022189,
     "end_time": "2024-03-26T16:01:12.763009",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.740820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df,exclude_columns = [], verbose=True):\n",
    "    import time\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_time = time.time()\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        if col in exclude_columns:\n",
    "            continue\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        print('reduce memory use:',round(time.time() - start_time,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e463597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:12.777611Z",
     "iopub.status.busy": "2024-03-26T16:01:12.777341Z",
     "iopub.status.idle": "2024-03-26T16:01:13.474927Z",
     "shell.execute_reply": "2024-03-26T16:01:13.474115Z"
    },
    "papermill": {
     "duration": 0.707686,
     "end_time": "2024-03-26T16:01:13.477447",
     "exception": false,
     "start_time": "2024-03-26T16:01:12.769761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_df = pd.read_feather(\"/kaggle/input/optiver/train_labels.fer\")\n",
    "target_df = target_df[target_df['date_id'] >= 400].reset_index(drop=True)\n",
    "gc.collect()\n",
    "# ä¸ºäº†ç»™ä¸€ä¸ªè¡¨å¤´\n",
    "target_df_save_retrain = target_df[target_df['date_id'] >= 478].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3d40d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:13.493288Z",
     "iopub.status.busy": "2024-03-26T16:01:13.493027Z",
     "iopub.status.idle": "2024-03-26T16:01:13.500372Z",
     "shell.execute_reply": "2024-03-26T16:01:13.499551Z"
    },
    "papermill": {
     "duration": 0.016918,
     "end_time": "2024-03-26T16:01:13.502215",
     "exception": false,
     "start_time": "2024-03-26T16:01:13.485297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrain_xgb(train_feas, feas_list, seed):\n",
    "    params = {\n",
    "        'random_state': seed,\n",
    "        'learning_rate':0.01,\n",
    "        'n_estimators':3200,\n",
    "        'n_jobs':-1,\n",
    "        'objective':'reg:absoluteerror',\n",
    "        \"device\": \"gpu\",\n",
    "        'max_depth': 10,\n",
    "         'min_child_weight': 8.860379669551103,\n",
    "         'subsample': 0.7711820080525443,\n",
    "         'colsample_bytree': 0.5348780216605801,\n",
    "         'reg_alpha': 0.12854342791716195,\n",
    "         'reg_lambda': 0.39326076062073634,\n",
    "         'gamma': 0.24378704040107024\n",
    "    }\n",
    "    date_ids = np.array(train_feas[\"date_id\"])\n",
    "    max_date = max(date_ids)\n",
    "    weights_date = np.ones_like(date_ids).astype(float)\n",
    "    weights_date[date_ids>=(max_date - 45)] = 1.5\n",
    "    del date_ids\n",
    "    gc.collect()\n",
    "\n",
    "    clf = xgb.XGBRegressor(**params)\n",
    "    clf.fit(train_feas[feas_list],train_feas['target'],sample_weight=weights_date)\n",
    "    gc.collect()\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4534297a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:01:13.517383Z",
     "iopub.status.busy": "2024-03-26T16:01:13.517130Z",
     "iopub.status.idle": "2024-03-26T16:02:46.686142Z",
     "shell.execute_reply": "2024-03-26T16:02:46.685059Z"
    },
    "papermill": {
     "duration": 93.179131,
     "end_time": "2024-03-26T16:02:46.688228",
     "exception": false,
     "start_time": "2024-03-26T16:01:13.509097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "counter: 0\n",
      "counter: 1\n",
      "counter: 2\n",
      "counter: 3\n",
      "counter: 4\n",
      "counter: 5\n",
      "counter: 6\n",
      "counter: 7\n",
      "counter: 8\n",
      "counter: 9\n",
      "10 queries per second: 0.3350233793258667\n",
      "counter: 10\n",
      "counter: 11\n",
      "counter: 12\n",
      "counter: 13\n",
      "counter: 14\n",
      "counter: 15\n",
      "counter: 16\n",
      "counter: 17\n",
      "counter: 18\n",
      "counter: 19\n",
      "20 queries per second: 0.3465649724006653\n",
      "counter: 20\n",
      "counter: 21\n",
      "counter: 22\n",
      "counter: 23\n",
      "counter: 24\n",
      "counter: 25\n",
      "counter: 26\n",
      "counter: 27\n",
      "counter: 28\n",
      "counter: 29\n",
      "30 queries per second: 0.3625314235687256\n",
      "counter: 30\n",
      "counter: 31\n",
      "counter: 32\n",
      "counter: 33\n",
      "counter: 34\n",
      "counter: 35\n",
      "counter: 36\n",
      "counter: 37\n",
      "counter: 38\n",
      "counter: 39\n",
      "40 queries per second: 0.3742712318897247\n",
      "counter: 40\n",
      "counter: 41\n",
      "counter: 42\n",
      "counter: 43\n",
      "counter: 44\n",
      "counter: 45\n",
      "counter: 46\n",
      "counter: 47\n",
      "counter: 48\n",
      "counter: 49\n",
      "50 queries per second: 0.3839058828353882\n",
      "counter: 50\n",
      "counter: 51\n",
      "counter: 52\n",
      "counter: 53\n",
      "counter: 54\n",
      "counter: 55\n",
      "counter: 56\n",
      "counter: 57\n",
      "counter: 58\n",
      "counter: 59\n",
      "60 queries per second: 0.3831640362739563\n",
      "counter: 60\n",
      "counter: 61\n",
      "counter: 62\n",
      "counter: 63\n",
      "counter: 64\n",
      "counter: 65\n",
      "counter: 66\n",
      "counter: 67\n",
      "counter: 68\n",
      "counter: 69\n",
      "70 queries per second: 0.37864602974482947\n",
      "counter: 70\n",
      "counter: 71\n",
      "counter: 72\n",
      "counter: 73\n",
      "counter: 74\n",
      "counter: 75\n",
      "counter: 76\n",
      "counter: 77\n",
      "counter: 78\n",
      "counter: 79\n",
      "80 queries per second: 0.3785385638475418\n",
      "counter: 80\n",
      "counter: 81\n",
      "counter: 82\n",
      "counter: 83\n",
      "counter: 84\n",
      "counter: 85\n",
      "counter: 86\n",
      "counter: 87\n",
      "counter: 88\n",
      "counter: 89\n",
      "90 queries per second: 0.38218315442403156\n",
      "counter: 90\n",
      "counter: 91\n",
      "counter: 92\n",
      "counter: 93\n",
      "counter: 94\n",
      "counter: 95\n",
      "counter: 96\n",
      "counter: 97\n",
      "counter: 98\n",
      "counter: 99\n",
      "100 queries per second: 0.3852199959754944\n",
      "counter: 100\n",
      "counter: 101\n",
      "counter: 102\n",
      "counter: 103\n",
      "counter: 104\n",
      "counter: 105\n",
      "counter: 106\n",
      "counter: 107\n",
      "counter: 108\n",
      "counter: 109\n",
      "110 queries per second: 0.3895421895113858\n",
      "counter: 110\n",
      "counter: 111\n",
      "counter: 112\n",
      "counter: 113\n",
      "counter: 114\n",
      "counter: 115\n",
      "counter: 116\n",
      "counter: 117\n",
      "counter: 118\n",
      "counter: 119\n",
      "120 queries per second: 0.38409310777982075\n",
      "counter: 120\n",
      "counter: 121\n",
      "counter: 122\n",
      "counter: 123\n",
      "counter: 124\n",
      "counter: 125\n",
      "counter: 126\n",
      "counter: 127\n",
      "counter: 128\n",
      "counter: 129\n",
      "130 queries per second: 0.3820103883743286\n",
      "counter: 130\n",
      "counter: 131\n",
      "counter: 132\n",
      "counter: 133\n",
      "counter: 134\n",
      "counter: 135\n",
      "counter: 136\n",
      "counter: 137\n",
      "counter: 138\n",
      "counter: 139\n",
      "140 queries per second: 0.3826966268675668\n",
      "counter: 140\n",
      "counter: 141\n",
      "counter: 142\n",
      "counter: 143\n",
      "counter: 144\n",
      "counter: 145\n",
      "counter: 146\n",
      "counter: 147\n",
      "counter: 148\n",
      "counter: 149\n",
      "150 queries per second: 0.38519389311472574\n",
      "counter: 150\n",
      "counter: 151\n",
      "counter: 152\n",
      "counter: 153\n",
      "counter: 154\n",
      "counter: 155\n",
      "counter: 156\n",
      "counter: 157\n",
      "counter: 158\n",
      "counter: 159\n",
      "160 queries per second: 0.3877143248915672\n",
      "counter: 160\n",
      "counter: 161\n",
      "counter: 162\n",
      "counter: 163\n",
      "counter: 164\n",
      "The code will take approximately 0.45 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import optiver2023\n",
    "env = optiver2023.make_env()  # Setting up the environment for the competition\n",
    "iter_test = env.iter_test()   # Getting the iterator for the test set\n",
    "\n",
    "counter = 0                   # Initializing a counter\n",
    "qps = []                      # Queries per second tracking\n",
    "y_min, y_max = -64, 64\n",
    "target_df_back_days = 68\n",
    "\n",
    "hist_df = pd.DataFrame()\n",
    "max_date = -1\n",
    "\n",
    "# å­˜æ”¾åŽé¢çš„labels\n",
    "save_labels_df_list = []\n",
    "save_feas_df_list = []\n",
    "train_data_saving_flag = True\n",
    "retrain_flag = False # çŽ°åœ¨å˜æˆæ˜¯å¦ç¬¬ä¸€æ¬¡è®­ç»ƒå®Œçš„æ ‡å¿—äº†\n",
    "is_first_scored = False\n",
    "scored_count = 0\n",
    "ready_to_second_retrain_flag = False\n",
    "\n",
    "for (test_df, revealed_targets, sample_prediction_df) in iter_test:\n",
    "\n",
    "    print('counter:', counter)\n",
    "    # å¤„ç†ä¸‹æ ¼å¼ target_df_tmpæ˜¯å‰ä¸€å¤©çš„label\n",
    "    test_df, target_df_tmp =  handle_format(test_df, revealed_targets)\n",
    "    \n",
    "    current_is_score = list(test_df['currently_scored'])[0]\n",
    "    \n",
    "    if (not is_first_scored) and current_is_score:\n",
    "        is_first_scored = True\n",
    "\n",
    "\n",
    "    test_df = test_df.drop('currently_scored', axis=1)\n",
    "    test_df[\"weight\"] = test_df[\"stock_id\"].map(weights)\n",
    "    current_date = test_df['date_id'].max()\n",
    "    current_second = test_df['seconds_in_bucket'].min()\n",
    "    \n",
    "    if current_is_score and current_date!=max_date:\n",
    "        scored_count +=1\n",
    "    if scored_count >= 30 and retrain_flag:\n",
    "        ready_to_second_retrain_flag = True\n",
    "\n",
    "    #481æ‰ä¼šå¢žåŠ åˆ°target_dfä¸Š åªæœ‰åœ¨æ–°æ—¥æœŸæ‰ä¼šæœ‰è¿™ä¸ªæ•°æ®\n",
    "    if current_date >=482 and current_date!=max_date:\n",
    "        target_df = pd.concat([target_df,target_df_tmp],ignore_index=True).sort_values(['date_id','seconds_in_bucket','stock_id'])\n",
    "        target_df = target_df[target_df['date_id']>=(current_date-target_df_back_days)].reset_index(drop=True) #åªå–top80å¤©å°±å¤Ÿäº†\n",
    "        gc.collect()\n",
    "\n",
    "        # (retrain)å­˜æ”¾æ–°æ•°æ®çš„labels\n",
    "        if len(target_df_tmp) > 0 and (train_data_saving_flag):\n",
    "            save_labels_df_list.append(target_df_tmp)\n",
    "            \n",
    "        # ç¬¬ä¸€æ¬¡è®­ç»ƒ\n",
    "        if (current_date >=560) and (is_first_scored) and (not retrain_flag) and (not ready_to_second_retrain_flag):\n",
    "            # æž„å»ºfeas\n",
    "            retrain_feas = pd.concat(save_feas_df_list,ignore_index=True)\n",
    "            # åˆ é™¤è¿™ä¸ªå˜é‡\n",
    "            del save_feas_df_list\n",
    "            save_feas_df_list = []\n",
    "            gc.collect()\n",
    "            # ç‰¹å¾å‡½æ•°\n",
    "            retrain_feas = reduce_mem_usage(retrain_feas, exclude_columns=['stock_id','date_id','seconds_in_bucket'])\n",
    "            gc.collect()\n",
    "            # train 0 -500å¤©çš„\n",
    "            labels_df_retrain = pd.concat(save_labels_df_list,ignore_index=True)\n",
    "            # è¡¨å¤´\n",
    "            labels_df_retrain = pd.concat([target_df_save_retrain, labels_df_retrain],ignore_index=True).sort_values(['date_id','seconds_in_bucket','stock_id'])\n",
    "            labels_df_retrain = labels_df_retrain[~labels_df_retrain['target'].isna()]\n",
    "            # å…³è”labels\n",
    "            retrain_feas = retrain_feas.merge(labels_df_retrain, how='inner',on=['stock_id','date_id','seconds_in_bucket'])\n",
    "            # æ¸…ç†å†…å­˜\n",
    "            del save_labels_df_list\n",
    "            save_labels_df_list = []                \n",
    "            del labels_df_retrain\n",
    "            # del target_df_save_retrain\n",
    "            gc.collect()\n",
    "            # åˆèµ·æ¥\n",
    "            retrain_feas = pd.concat([pd.read_feather(\"/kaggle/input/optiver/train_480_feas_drop60.fer\"), retrain_feas], ignore_index=True)\n",
    "            gc.collect()\n",
    "            retrain_feas = reduce_mem_usage(retrain_feas, exclude_columns=['stock_id','date_id','seconds_in_bucket'])\n",
    "            gc.collect()\n",
    "            new_xgb_models_list = []\n",
    "            # è¦åŠ ç§å­\n",
    "            for seed_ in [47,1103,2023]:\n",
    "                gc.collect()\n",
    "                new_xgb_models_list.append(retrain_xgb(retrain_feas, selected_feas, seed_))\n",
    "            gc.collect()\n",
    "            # æ¸…ç†æ—§æ–‡ä»¶\n",
    "            # del retrain_feas\n",
    "            retrain_feas = retrain_feas[retrain_feas['date_id']>=90].reset_index(drop=True)\n",
    "            gc.collect()\n",
    "            retrain_flag = True\n",
    "            #train_data_saving_flag = False\n",
    "            scored_count = 0\n",
    "            \n",
    "        # ç¬¬äºŒæ¬¡è®­ç»ƒ\n",
    "        if (current_date > 565) and (retrain_flag) and (ready_to_second_retrain_flag) and (train_data_saving_flag):\n",
    "            # æž„å»ºfeas\n",
    "            retrain_feas_2 = pd.concat(save_feas_df_list,ignore_index=True)\n",
    "            # åˆ é™¤è¿™ä¸ªå˜é‡\n",
    "            del save_feas_df_list\n",
    "            save_feas_df_list = []\n",
    "            gc.collect()\n",
    "            retrain_feas_2 = reduce_mem_usage(retrain_feas_2, exclude_columns=['stock_id','date_id','seconds_in_bucket'])\n",
    "            gc.collect()\n",
    "            # train 0 -500å¤©çš„\n",
    "            labels_df_retrain = pd.concat(save_labels_df_list,ignore_index=True)\n",
    "            # è¡¨å¤´\n",
    "            labels_df_retrain = pd.concat([target_df_save_retrain, labels_df_retrain],ignore_index=True).sort_values(['date_id','seconds_in_bucket','stock_id'])\n",
    "            labels_df_retrain = labels_df_retrain[~labels_df_retrain['target'].isna()]\n",
    "            # å…³è”labels\n",
    "            retrain_feas_2 = retrain_feas_2.merge(labels_df_retrain, how='inner',on=['stock_id','date_id','seconds_in_bucket'])\n",
    "            # æ¸…ç†å†…å­˜\n",
    "            del save_labels_df_list\n",
    "            save_labels_df_list = []                \n",
    "            del labels_df_retrain\n",
    "            del target_df_save_retrain\n",
    "            gc.collect()\n",
    "            # åˆèµ·æ¥\n",
    "            retrain_feas = pd.concat([retrain_feas, retrain_feas_2], ignore_index=True)\n",
    "            del retrain_feas_2\n",
    "            gc.collect()\n",
    "            retrain_feas = reduce_mem_usage(retrain_feas)\n",
    "            gc.collect()\n",
    "            new_xgb_models_list = []\n",
    "            # è¦åŠ ç§å­\n",
    "            for seed_ in [47,1103,2023]:\n",
    "                gc.collect()\n",
    "                new_xgb_models_list.append(retrain_xgb(retrain_feas, selected_feas, seed_))\n",
    "            # æ¸…ç†æ—§æ–‡ä»¶\n",
    "            del retrain_feas\n",
    "            gc.collect()\n",
    "            #retrain_flag = True\n",
    "            train_data_saving_flag = False     \n",
    "\n",
    "\n",
    "    # åštargetç›¸å…³ç‰¹å¾\n",
    "    test_df_mock = test_df[['stock_id','date_id','seconds_in_bucket']].copy()\n",
    "    current_target_feas_polars = get_target_feathers(target_df,current_date,current_second,test_df_mock) # polars\n",
    "\n",
    "\n",
    "\n",
    "    now_time = time.time()    # Current time for performance measurement\n",
    "    if current_date != max_date:\n",
    "        hist_df = pd.DataFrame()  \n",
    "\n",
    "    hist_df = pd.concat([hist_df,test_df],ignore_index=True)\n",
    "\n",
    "\n",
    "    pred_df, _ = generate_features_no_hist_polars(hist_df,current_target_feas_polars)\n",
    "    pred_df = test_df[['stock_id','date_id','seconds_in_bucket']].merge(pred_df,how='left',on=['stock_id','date_id','seconds_in_bucket'])\n",
    "\n",
    "    # å¤„ç†na/clip\n",
    "    pred_df = pred_df.fillna(-9e10)\n",
    "    for _ in selected_feas:\n",
    "        pred_df[_] = pred_df[_].clip(lower=-9e9,upper=9e9)\n",
    "\n",
    "    #(retrain)\n",
    "    if train_data_saving_flag and current_date >= 481:\n",
    "        save_feas_df_list.append(pred_df)\n",
    "\n",
    "    if current_is_score:\n",
    "        xgb_pred_list = []\n",
    "\n",
    "        if retrain_flag and len(new_xgb_models_list) > 0:\n",
    "            for model in new_xgb_models_list:\n",
    "                xgb_pred_list.append(model.predict(pred_df[selected_feas]))\n",
    "        else:\n",
    "            for model in xgb_model_list:\n",
    "                xgb_pred_list.append(model.predict(pred_df[selected_feas]))\n",
    "\n",
    "\n",
    "        #åŽå¤„ç†\n",
    "        lgb_predictions = np.mean(xgb_pred_list,axis=0)\n",
    "        test_df['pred'] = lgb_predictions\n",
    "        test_df['w_pred'] = test_df['weight'] * test_df['pred']\n",
    "        test_df[\"post_num\"] = test_df.groupby([\"date_id\",\"seconds_in_bucket\"])['w_pred'].transform('sum') / test_df.groupby([\"date_id\",\"seconds_in_bucket\"])['weight'].transform('sum')\n",
    "        test_df['pred'] = test_df['pred'] - test_df['post_num']\n",
    "\n",
    "\n",
    "        sample_prediction_df['target'] = list(test_df['pred'])\n",
    "    else:\n",
    "        sample_prediction_df['target'] = 0\n",
    "\n",
    "\n",
    "    # Use the environment to make predictions\n",
    "    env.predict(sample_prediction_df)\n",
    "\n",
    "    max_date = test_df['date_id'].max()\n",
    "\n",
    "    counter += 1\n",
    "    qps.append(time.time() - now_time)\n",
    "    gc.collect()\n",
    "    if counter % 10 == 0:\n",
    "        print(f\"{counter} queries per second: {np.mean(qps)}\")\n",
    "#     except:\n",
    "#         sample_prediction_df['target'] = 0\n",
    "#         env.predict(sample_prediction_df)\n",
    "#         print(\"error********************************************\")\n",
    "\n",
    "time_cost = 1.146 * np.mean(qps)\n",
    "print(f\"The code will take approximately {np.round(time_cost, 2)} hours to reason about\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d45ae53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:02:46.729697Z",
     "iopub.status.busy": "2024-03-26T16:02:46.729401Z",
     "iopub.status.idle": "2024-03-26T16:02:46.744250Z",
     "shell.execute_reply": "2024-03-26T16:02:46.743180Z"
    },
    "papermill": {
     "duration": 0.037404,
     "end_time": "2024-03-26T16:02:46.746230",
     "exception": false,
     "start_time": "2024-03-26T16:02:46.708826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480_540_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480_540_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480_540_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480_540_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480_540_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>480_540_195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>480_540_196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>480_540_197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>480_540_198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>480_540_199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id  target\n",
       "0      480_540_0       0\n",
       "1      480_540_1       0\n",
       "2      480_540_2       0\n",
       "3      480_540_3       0\n",
       "4      480_540_4       0\n",
       "..           ...     ...\n",
       "195  480_540_195       0\n",
       "196  480_540_196       0\n",
       "197  480_540_197       0\n",
       "198  480_540_198       0\n",
       "199  480_540_199       0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b56271b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:02:46.786631Z",
     "iopub.status.busy": "2024-03-26T16:02:46.786341Z",
     "iopub.status.idle": "2024-03-26T16:02:46.789940Z",
     "shell.execute_reply": "2024-03-26T16:02:46.789130Z"
    },
    "papermill": {
     "duration": 0.026189,
     "end_time": "2024-03-26T16:02:46.791880",
     "exception": false,
     "start_time": "2024-03-26T16:02:46.765691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e21f6850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T16:02:46.832076Z",
     "iopub.status.busy": "2024-03-26T16:02:46.831791Z",
     "iopub.status.idle": "2024-03-26T16:02:46.835505Z",
     "shell.execute_reply": "2024-03-26T16:02:46.834681Z"
    },
    "papermill": {
     "duration": 0.025882,
     "end_time": "2024-03-26T16:02:46.837537",
     "exception": false,
     "start_time": "2024-03-26T16:02:46.811655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -4.576492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdba424",
   "metadata": {
    "papermill": {
     "duration": 0.019446,
     "end_time": "2024-03-26T16:02:46.876508",
     "exception": false,
     "start_time": "2024-03-26T16:02:46.857062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4057701,
     "isSourceIdPinned": false,
     "sourceId": 7240192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 153.353254,
   "end_time": "2024-03-26T16:02:48.217514",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-26T16:00:14.864260",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
