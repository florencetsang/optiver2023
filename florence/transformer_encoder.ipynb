{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ba5f06-d483-4625-8b0e-101f05afcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d284441-83ea-4b5e-bbc2-df7b7f23e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split, InputDataset, LabelDataset\n",
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")\n",
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from gluonts.itertools import Cyclic, Cached\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Optional, Iterable, Sized, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355f5fbb-d3a4-4aff-982c-978c2fcf194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d8b1d3-8a36-4f8c-8f3f-d92e295a4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLogger:\n",
    "    def log(self, msg):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class BasicModelLogger(ModelLogger):\n",
    "    def __init__(self, msg_prefix):\n",
    "        self.msg_prefix = msg_prefix\n",
    "        # self.log_idx = 0\n",
    "    \n",
    "    def log(self, msg):\n",
    "        print(f\"{self.msg_prefix} - {msg}\")\n",
    "        # self.log_idx += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        # self.log_idx = 0\n",
    "        pass\n",
    "\n",
    "class NoopModelLogger(ModelLogger):\n",
    "    def log(self, msg):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8b75b-6d45-48ad-9930-16263cafbabe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc303388-ea46-4812-808d-9ffd9984fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_classes: int,\n",
    "        embedding_dim: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float,\n",
    "        logger: ModelLogger,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.input_ff = nn.Linear(num_input_features + embedding_dim, d_model)\n",
    "        self.input_ff_sigmoid = nn.Sigmoid()\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.final_linear = nn.Linear(d_model, 1)\n",
    "        self.init_weights()\n",
    "        self.logger = logger\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.input_ff.bias.data.zero_()\n",
    "        self.input_ff.weight.data.uniform_(-initrange, initrange)\n",
    "        self.final_linear.bias.data.zero_()\n",
    "        self.final_linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        item_ids: Tensor,\n",
    "        src_mask: Tensor = None,\n",
    "    ) -> Tensor:\n",
    "        # src: [batch_size b, seq_len k 55, features 27]\n",
    "        batch_size = src.size(dim=0)\n",
    "        seq_len = src.size(dim=1)\n",
    "        num_input_features = src.size(dim=2)\n",
    "        output = src\n",
    "        embedded = self.embedding(item_ids)\n",
    "        # idea from https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py#L1290\n",
    "        embedded = embedded.unsqueeze(dim=1).expand(-1, seq_len, -1)\n",
    "\n",
    "        output = torch.cat((src, embedded), dim=-1)\n",
    "        self.logger.log(f\"{output.size()}\")\n",
    "        \n",
    "        output = self.input_ff(output)\n",
    "        self.logger.log(f\"input_ff - {output.size()}\")\n",
    "        # output = self.input_ff_sigmoid(output)\n",
    "        # self.logger.log(f\"input_ff_sigmoid - {output.size()}\")\n",
    "        \n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(src.size(dim=1)).to(device)\n",
    "        self.logger.log(f\"src_mask - {src_mask.size()}\")\n",
    "        \n",
    "        output = self.transformer_encoder(output, src_mask)\n",
    "        self.logger.log(f\"encoder - {output.size()}\")\n",
    "        \n",
    "        output = self.final_linear(output)\n",
    "        self.logger.log(f\"final_linear - {output.size()}\")\n",
    "        \n",
    "        output = output[:, -1, :]\n",
    "        output = output.squeeze(dim=1)\n",
    "        self.logger.log(f\"output - {output.size()}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b71edd-ebc9-41f5-8f80-984b55e48674",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5316823f-0a3f-4dfd-ad2e-21944357b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"optiver-trading-at-the-close/train.csv\",\n",
    "    dtype={\n",
    "        \"imbalance_size\": np.float32,\n",
    "        \"reference_price\": np.float32,\n",
    "        \"matched_size\": np.float32,\n",
    "    },\n",
    ")\n",
    "raw_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2294ef50-c777-4ace-8624-8e966ba0f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d6efd1-8e47-4daa-a597-dda8fd29abbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.50</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.04</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.00</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.90</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.010200</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.54</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237975</th>\n",
       "      <td>195</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>2.440723e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>28280362.00</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>32257.04</td>\n",
       "      <td>1.000434</td>\n",
       "      <td>319862.40</td>\n",
       "      <td>1.000328</td>\n",
       "      <td>2.310276</td>\n",
       "      <td>26454</td>\n",
       "      <td>480_540_195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237976</th>\n",
       "      <td>196</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>3.495105e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>9187699.00</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>1.000386</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>205108.40</td>\n",
       "      <td>1.000900</td>\n",
       "      <td>93393.07</td>\n",
       "      <td>1.000819</td>\n",
       "      <td>-8.220077</td>\n",
       "      <td>26454</td>\n",
       "      <td>480_540_196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237977</th>\n",
       "      <td>197</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>12725436.00</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>0.995789</td>\n",
       "      <td>16790.66</td>\n",
       "      <td>0.995883</td>\n",
       "      <td>180038.32</td>\n",
       "      <td>0.995797</td>\n",
       "      <td>1.169443</td>\n",
       "      <td>26454</td>\n",
       "      <td>480_540_197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237978</th>\n",
       "      <td>198</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.000899e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>94773272.00</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>125631.72</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>669893.00</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>-1.540184</td>\n",
       "      <td>26454</td>\n",
       "      <td>480_540_198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237979</th>\n",
       "      <td>199</td>\n",
       "      <td>480</td>\n",
       "      <td>540</td>\n",
       "      <td>1.884286e+06</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>24073678.00</td>\n",
       "      <td>1.000859</td>\n",
       "      <td>1.001494</td>\n",
       "      <td>1.002129</td>\n",
       "      <td>250081.44</td>\n",
       "      <td>1.002447</td>\n",
       "      <td>300167.56</td>\n",
       "      <td>1.002274</td>\n",
       "      <td>-6.530285</td>\n",
       "      <td>26454</td>\n",
       "      <td>480_540_199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5237980 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0               0        0                  0    3.180603e+06   \n",
       "1               1        0                  0    1.666039e+05   \n",
       "2               2        0                  0    3.028799e+05   \n",
       "3               3        0                  0    1.191768e+07   \n",
       "4               4        0                  0    4.475500e+05   \n",
       "...           ...      ...                ...             ...   \n",
       "5237975       195      480                540    2.440723e+06   \n",
       "5237976       196      480                540    3.495105e+05   \n",
       "5237977       197      480                540    0.000000e+00   \n",
       "5237978       198      480                540    1.000899e+06   \n",
       "5237979       199      480                540    1.884286e+06   \n",
       "\n",
       "         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                              1         0.999812   13380277.00        NaN   \n",
       "1                             -1         0.999896    1642214.25        NaN   \n",
       "2                             -1         0.999561    1819368.00        NaN   \n",
       "3                             -1         1.000171   18389746.00        NaN   \n",
       "4                             -1         0.999532   17860614.00        NaN   \n",
       "...                          ...              ...           ...        ...   \n",
       "5237975                       -1         1.000317   28280362.00   0.999734   \n",
       "5237976                       -1         1.000643    9187699.00   1.000129   \n",
       "5237977                        0         0.995789   12725436.00   0.995789   \n",
       "5237978                        1         0.999210   94773272.00   0.999210   \n",
       "5237979                       -1         1.002129   24073678.00   1.000859   \n",
       "\n",
       "         near_price  bid_price   bid_size  ask_price   ask_size       wap  \\\n",
       "0               NaN   0.999812   60651.50   1.000026    8493.03  1.000000   \n",
       "1               NaN   0.999896    3233.04   1.000660   20605.09  1.000000   \n",
       "2               NaN   0.999403   37956.00   1.000298   18995.00  1.000000   \n",
       "3               NaN   0.999999    2324.90   1.000214  479032.40  1.000000   \n",
       "4               NaN   0.999394   16485.54   1.000016     434.10  1.000000   \n",
       "...             ...        ...        ...        ...        ...       ...   \n",
       "5237975    0.999734   1.000317   32257.04   1.000434  319862.40  1.000328   \n",
       "5237976    1.000386   1.000643  205108.40   1.000900   93393.07  1.000819   \n",
       "5237977    0.995789   0.995789   16790.66   0.995883  180038.32  0.995797   \n",
       "5237978    0.999210   0.998970  125631.72   0.999210  669893.00  0.999008   \n",
       "5237979    1.001494   1.002129  250081.44   1.002447  300167.56  1.002274   \n",
       "\n",
       "           target  time_id       row_id  \n",
       "0       -3.029704        0        0_0_0  \n",
       "1       -5.519986        0        0_0_1  \n",
       "2       -8.389950        0        0_0_2  \n",
       "3       -4.010200        0        0_0_3  \n",
       "4       -7.349849        0        0_0_4  \n",
       "...           ...      ...          ...  \n",
       "5237975  2.310276    26454  480_540_195  \n",
       "5237976 -8.220077    26454  480_540_196  \n",
       "5237977  1.169443    26454  480_540_197  \n",
       "5237978 -1.540184    26454  480_540_198  \n",
       "5237979 -6.530285    26454  480_540_199  \n",
       "\n",
       "[5237980 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f86ef-d1a7-4fd9-b4fb-51317ad6aa43",
   "metadata": {},
   "source": [
    "## Data pre-processing and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b3c19b-0507-41c6-883b-f494a8f33f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dynamic_real = [\"imbalance_size\", \"reference_price\", \"matched_size\"]\n",
    "# feat_dynamic_real = []\n",
    "num_input_features = len(feat_dynamic_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7671f-c808-4a8d-9ed4-af83242669a3",
   "metadata": {},
   "source": [
    "## Add time index to data (for gluonts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ba3392-e0da-4c6a-999c-66bf2880fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 00:01:00',\n",
      "               '2018-01-01 00:02:00', '2018-01-01 00:03:00',\n",
      "               '2018-01-01 00:04:00', '2018-01-01 00:05:00',\n",
      "               '2018-01-01 00:06:00', '2018-01-01 00:07:00',\n",
      "               '2018-01-01 00:08:00', '2018-01-01 00:09:00',\n",
      "               ...\n",
      "               '2018-01-19 08:45:00', '2018-01-19 08:46:00',\n",
      "               '2018-01-19 08:47:00', '2018-01-19 08:48:00',\n",
      "               '2018-01-19 08:49:00', '2018-01-19 08:50:00',\n",
      "               '2018-01-19 08:51:00', '2018-01-19 08:52:00',\n",
      "               '2018-01-19 08:53:00', '2018-01-19 08:54:00'],\n",
      "              dtype='datetime64[ns]', length=26455, freq='T')\n",
      "PeriodIndex(['2018-01-01 00:00', '2018-01-01 00:01', '2018-01-01 00:02',\n",
      "             '2018-01-01 00:03', '2018-01-01 00:04', '2018-01-01 00:05',\n",
      "             '2018-01-01 00:06', '2018-01-01 00:07', '2018-01-01 00:08',\n",
      "             '2018-01-01 00:09',\n",
      "             ...\n",
      "             '2018-01-19 08:45', '2018-01-19 08:46', '2018-01-19 08:47',\n",
      "             '2018-01-19 08:48', '2018-01-19 08:49', '2018-01-19 08:50',\n",
      "             '2018-01-19 08:51', '2018-01-19 08:52', '2018-01-19 08:53',\n",
      "             '2018-01-19 08:54'],\n",
      "            dtype='period[T]', length=26455)\n",
      "0        2018-01-01 00:00\n",
      "1        2018-01-01 00:01\n",
      "2        2018-01-01 00:02\n",
      "3        2018-01-01 00:03\n",
      "4        2018-01-01 00:04\n",
      "               ...       \n",
      "26450    2018-01-19 08:50\n",
      "26451    2018-01-19 08:51\n",
      "26452    2018-01-19 08:52\n",
      "26453    2018-01-19 08:53\n",
      "26454    2018-01-19 08:54\n",
      "Length: 26455, dtype: period[T]\n"
     ]
    }
   ],
   "source": [
    "max_time_id = df[\"time_id\"].max()\n",
    "dti_by_time_id = pd.date_range(\"2018-01-01\", periods=max_time_id + 1, freq=\"min\")\n",
    "print(dti_by_time_id)\n",
    "df_period_index_by_time_id = dti_by_time_id.to_period(\"1min\")\n",
    "print(df_period_index_by_time_id)\n",
    "df_index_by_time_id_series = df_period_index_by_time_id.to_series(index=np.arange(max_time_id + 1))\n",
    "print(df_index_by_time_id_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c9a0c3e-d713-4036-a3bb-1cfacd00438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp_by_time_id\"] = df[\"time_id\"].map(df_index_by_time_id_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd73b3d-3f52-4ed3-b793-18bd6fcd235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = df[\"timestamp_by_time_id\"]\n",
    "df.index = df[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88018152-daca-4bfe-a002-c101b12e2858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>timestamp_by_time_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.50</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_0</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.04</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_1</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.00</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_2</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.90</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.010200</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_3</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.54</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_4</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "      <td>2018-01-01 00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "timestamp                                                                \n",
       "2018-01-01 00:00         0        0                  0    3.180603e+06   \n",
       "2018-01-01 00:00         1        0                  0    1.666039e+05   \n",
       "2018-01-01 00:00         2        0                  0    3.028799e+05   \n",
       "2018-01-01 00:00         3        0                  0    1.191768e+07   \n",
       "2018-01-01 00:00         4        0                  0    4.475500e+05   \n",
       "\n",
       "                  imbalance_buy_sell_flag  reference_price  matched_size  \\\n",
       "timestamp                                                                  \n",
       "2018-01-01 00:00                        1         0.999812   13380277.00   \n",
       "2018-01-01 00:00                       -1         0.999896    1642214.25   \n",
       "2018-01-01 00:00                       -1         0.999561    1819368.00   \n",
       "2018-01-01 00:00                       -1         1.000171   18389746.00   \n",
       "2018-01-01 00:00                       -1         0.999532   17860614.00   \n",
       "\n",
       "                  far_price  near_price  bid_price  bid_size  ask_price  \\\n",
       "timestamp                                                                 \n",
       "2018-01-01 00:00        NaN         NaN   0.999812  60651.50   1.000026   \n",
       "2018-01-01 00:00        NaN         NaN   0.999896   3233.04   1.000660   \n",
       "2018-01-01 00:00        NaN         NaN   0.999403  37956.00   1.000298   \n",
       "2018-01-01 00:00        NaN         NaN   0.999999   2324.90   1.000214   \n",
       "2018-01-01 00:00        NaN         NaN   0.999394  16485.54   1.000016   \n",
       "\n",
       "                   ask_size  wap    target  time_id row_id  \\\n",
       "timestamp                                                    \n",
       "2018-01-01 00:00    8493.03  1.0 -3.029704        0  0_0_0   \n",
       "2018-01-01 00:00   20605.09  1.0 -5.519986        0  0_0_1   \n",
       "2018-01-01 00:00   18995.00  1.0 -8.389950        0  0_0_2   \n",
       "2018-01-01 00:00  479032.40  1.0 -4.010200        0  0_0_3   \n",
       "2018-01-01 00:00     434.10  1.0 -7.349849        0  0_0_4   \n",
       "\n",
       "                 timestamp_by_time_id         timestamp  \n",
       "timestamp                                                \n",
       "2018-01-01 00:00     2018-01-01 00:00  2018-01-01 00:00  \n",
       "2018-01-01 00:00     2018-01-01 00:00  2018-01-01 00:00  \n",
       "2018-01-01 00:00     2018-01-01 00:00  2018-01-01 00:00  \n",
       "2018-01-01 00:00     2018-01-01 00:00  2018-01-01 00:00  \n",
       "2018-01-01 00:00     2018-01-01 00:00  2018-01-01 00:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca31a49-8986-44e6-973b-1142edc97979",
   "metadata": {},
   "source": [
    "## Group by stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b7bd99-3e4b-4c1a-9fc2-07ccd9bf641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "stock_id\n",
      "0      26455\n",
      "1      26455\n",
      "2      26455\n",
      "3      26455\n",
      "4      26455\n",
      "       ...  \n",
      "195    26455\n",
      "196    26455\n",
      "197    26455\n",
      "198    26455\n",
      "199    21615\n",
      "Length: 200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_grouped = df.groupby(\"stock_id\")\n",
    "print(len(df_grouped))\n",
    "print(df_grouped.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9c599-96e7-4884-95c5-49b4c4da1592",
   "metadata": {},
   "source": [
    "## Create gluonts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ba5dd5-d4ef-41cb-af07-64a28532fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fefae59-b1f1-44ed-b3f1-f470d8b2b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict = {}\n",
    "for item_id, gdf in df_grouped:\n",
    "    dfs_dict[item_id] = gdf.reindex(df_index_by_time_id_series).drop(\"stock_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8fc802-0555-4021-a915-2bd098c20919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id, df in dfs_dict.items():\n",
    "    df.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3771e095-6ec6-413c-910d-a2640f54632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataset<size=200, freq=1min, num_feat_dynamic_real=3, num_past_feat_dynamic_real=0, num_feat_static_real=0, num_feat_static_cat=0, static_cardinalities=[]>\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dataset = PandasDataset(dfs_dict, target=\"target\", feat_dynamic_real=feat_dynamic_real, freq=freq, assume_sorted=False)\n",
    "print(dataset)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687dde16-8a06-40cd-b849-060c3eb9b727",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a21973c-3666-4842-a468-5860bdcc7b3a",
   "metadata": {},
   "source": [
    "### Pytorch dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5935edf-b5fe-4e3c-b267-74fee0cd676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_max_date_id = 480 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "827d32ac-2f16-47a8-804f-9a99c7f3bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 1\n",
    "seq_len = 55\n",
    "\n",
    "training_batch_size = 256\n",
    "validation_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e00037e5-af83-4f4b-9e89-b60accf6d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, stock_df, feature_names, target_col, item_id, context_length):\n",
    "        super().__init__()\n",
    "        self.features = stock_df[feature_names]\n",
    "        self.targets = stock_df[target_col]\n",
    "        self.item_id = item_id\n",
    "        self.context_length = context_length\n",
    "        self.total_size = self.features.shape[0] - context_length + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx\n",
    "        end_idx = idx + self.context_length\n",
    "        features = self.features.iloc[start_idx:end_idx]\n",
    "        targets = self.targets.iloc[start_idx:end_idx]\n",
    "        return features.values, self.item_id, targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7187d1a9-db84-4119-a115-d5dd8b4bbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLengthSequentialSampler(torch.utils.data.sampler.Sampler[int]):\n",
    "    data_source: Sized\n",
    "\n",
    "    def __init__(self, data_source: Sized, max_samples: int) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.num_samples = min(len(data_source), max_samples)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(range(self.num_samples))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf94625c-b4d2-4c11-9baa-b9a1f9f635fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200\n"
     ]
    }
   ],
   "source": [
    "stock_training_datasets = []\n",
    "stock_validation_datasets = []\n",
    "for item_id, gdf in df_grouped:\n",
    "    training_mask = gdf[\"date_id\"] < training_set_max_date_id\n",
    "    training_df = gdf[training_mask]\n",
    "    validation_df = gdf[~training_mask]\n",
    "    assert training_df.shape[0] > 0 \\\n",
    "        and validation_df.shape[0] > 0 \\\n",
    "        and training_df.shape[0] + validation_df.shape[0] == gdf.shape[0], f\"{item_id} invalid shape\"\n",
    "    stock_training_datasets.append(StockTrainingDataset(training_df, feat_dynamic_real, \"target\", item_id, seq_len))\n",
    "    stock_validation_datasets.append(StockTrainingDataset(validation_df, feat_dynamic_real, \"target\", item_id, seq_len))\n",
    "print(len(stock_training_datasets), len(stock_validation_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8daff433-f66a-45c6-800a-1b23bb97bc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4996180 220200\n"
     ]
    }
   ],
   "source": [
    "full_training_dataset = torch.utils.data.ConcatDataset(stock_training_datasets)\n",
    "full_validation_dataset = torch.utils.data.ConcatDataset(stock_validation_datasets)\n",
    "print(len(full_training_dataset), len(full_validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f648120-a83d-4f5c-b53e-f31851fb80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to RandomSampler\n",
    "training_sampler = CustomLengthSequentialSampler(full_training_dataset, 256 * 100)\n",
    "validation_sampler = torch.utils.data.SequentialSampler(full_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c607398d-2f29-4866-addb-e8b8a5053458",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(\n",
    "    full_training_dataset,\n",
    "    batch_size=training_batch_size,\n",
    "    sampler=training_sampler,\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    full_validation_dataset,\n",
    "    batch_size=validation_batch_size,\n",
    "    sampler=validation_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a822b02-69a4-4c33-8774-30890ab313f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 55, 3]) torch.Size([256]) torch.Size([256, 55])\n",
      "torch.Size([256, 55, 3]) torch.Size([256]) torch.Size([256, 55])\n"
     ]
    }
   ],
   "source": [
    "training_sample_batch = next(iter(training_dataloader))\n",
    "print(training_sample_batch[0].size(), training_sample_batch[1].size(), training_sample_batch[2].size())\n",
    "validation_sample_batch = next(iter(validation_dataloader))\n",
    "print(validation_sample_batch[0].size(), validation_sample_batch[1].size(), validation_sample_batch[2].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c86949-80fb-445d-abe5-71f0a87efa92",
   "metadata": {},
   "source": [
    "### Gluonts data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "730b0d8c-746b-466a-b494-dd33ee39dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "test_dataset_size = 55 * 20\n",
    "training_data, test_gen = split(dataset, offset=-test_dataset_size)\n",
    "test_dataset = test_gen.generate_instances(prediction_length=prediction_length, windows=int(test_dataset_size / prediction_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14f9f427-2d4d-48e6-8d02-7d516f81593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = training_data\n",
    "test_data_input_dataset = InputDataset(test_dataset)\n",
    "test_data_label_dataset = LabelDataset(test_dataset)\n",
    "\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "test_data_input_dataset_iter = iter(test_data_input_dataset)\n",
    "test_data_label_dataset_iter = iter(test_data_label_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9379e361-8480-4bbf-9c54-4aaa2c769a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_sample(sample):\n",
    "    print(f\"sample snippet: {sample}\")\n",
    "    print(f\"sample dict keys: {sample.keys()}\")\n",
    "    print(f'start: {sample[\"start\"]} {type(sample[\"start\"])}')\n",
    "    print(f'target shape: {type(sample[\"target\"])} {sample[\"target\"].shape}')\n",
    "    print(f'item id: {sample[\"item_id\"]}')\n",
    "    if \"feat_dynamic_real\" in sample:\n",
    "        print(f'feat_dynamic_real\": {type(sample[\"feat_dynamic_real\"])} {sample[\"feat_dynamic_real\"].shape} {sample[\"feat_dynamic_real\"].dtype}')\n",
    "    else:\n",
    "        print(\"no feat_dynamic_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c66202f-5cfb-4ff1-af44-b05b3ae2186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "sample snippet: {'start': Period('2018-01-01 00:00', 'T'), 'target': array([-3.029704  ,  0.38981438,  4.220009  , ..., -4.580021  ,\n",
      "       -6.740093  ,  0.61035156]), 'item_id': 0, 'feat_dynamic_real': array([[3.1806028e+06, 1.2997728e+06, 1.2997728e+06, ..., 3.3379682e+06,\n",
      "        3.3363655e+06, 3.3363655e+06],\n",
      "       [9.9981201e-01, 1.0000260e+00, 9.9991900e-01, ..., 1.0022500e+00,\n",
      "        1.0022500e+00, 1.0022500e+00],\n",
      "       [1.3380277e+07, 1.5261107e+07, 1.5261107e+07, ..., 4.8395436e+07,\n",
      "        4.8397036e+07, 4.8397036e+07]], dtype=float32)}\n",
      "sample dict keys: dict_keys(['start', 'target', 'item_id', 'feat_dynamic_real'])\n",
      "start: 2018-01-01 00:00 <class 'pandas._libs.tslibs.period.Period'>\n",
      "target shape: <class 'numpy.ndarray'> (25355,)\n",
      "item id: 0\n",
      "feat_dynamic_real\": <class 'numpy.ndarray'> (3, 25355) float32\n"
     ]
    }
   ],
   "source": [
    "train_sample = next(train_dataset_iter)\n",
    "print(len(training_data))\n",
    "print_data_sample(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7923743-0bf9-4ca7-9615-9a31deaf9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220000\n",
      "sample snippet: {'start': Period('2018-01-01 00:00', 'T'), 'target': array([-3.029704  ,  0.38981438,  4.220009  , ..., -4.580021  ,\n",
      "       -6.740093  ,  0.61035156]), 'item_id': 0, 'feat_dynamic_real': array([[3.1806028e+06, 1.2997728e+06, 1.2997728e+06, ..., 3.3363655e+06,\n",
      "        3.3363655e+06, 1.0247004e+06],\n",
      "       [9.9981201e-01, 1.0000260e+00, 9.9991900e-01, ..., 1.0022500e+00,\n",
      "        1.0022500e+00, 1.0003310e+00],\n",
      "       [1.3380277e+07, 1.5261107e+07, 1.5261107e+07, ..., 4.8397036e+07,\n",
      "        4.8397036e+07, 1.5222857e+07]], dtype=float32)}\n",
      "sample dict keys: dict_keys(['start', 'target', 'item_id', 'feat_dynamic_real'])\n",
      "start: 2018-01-01 00:00 <class 'pandas._libs.tslibs.period.Period'>\n",
      "target shape: <class 'numpy.ndarray'> (25355,)\n",
      "item id: 0\n",
      "feat_dynamic_real\": <class 'numpy.ndarray'> (3, 25356) float32\n",
      "220000\n",
      "sample snippet: {'start': Period('2018-01-18 14:35', 'T'), 'target': array([2.900362]), 'item_id': 0, 'feat_dynamic_real': array([[1.0247004e+06, 1.7997806e+06],\n",
      "       [1.0003310e+00, 9.9999303e-01],\n",
      "       [1.5222857e+07, 1.5224874e+07]], dtype=float32)}\n",
      "sample dict keys: dict_keys(['start', 'target', 'item_id', 'feat_dynamic_real'])\n",
      "start: 2018-01-18 14:35 <class 'pandas._libs.tslibs.period.Period'>\n",
      "target shape: <class 'numpy.ndarray'> (1,)\n",
      "item id: 0\n",
      "feat_dynamic_real\": <class 'numpy.ndarray'> (3, 2) float32\n",
      "----------\n",
      "220000\n",
      "sample snippet: {'start': Period('2018-01-01 00:00', 'T'), 'target': array([-3.029704  ,  0.38981438,  4.220009  , ..., -6.740093  ,\n",
      "        0.61035156,  2.900362  ]), 'item_id': 0, 'feat_dynamic_real': array([[3.1806028e+06, 1.2997728e+06, 1.2997728e+06, ..., 3.3363655e+06,\n",
      "        1.0247004e+06, 1.7997806e+06],\n",
      "       [9.9981201e-01, 1.0000260e+00, 9.9991900e-01, ..., 1.0022500e+00,\n",
      "        1.0003310e+00, 9.9999303e-01],\n",
      "       [1.3380277e+07, 1.5261107e+07, 1.5261107e+07, ..., 4.8397036e+07,\n",
      "        1.5222857e+07, 1.5224874e+07]], dtype=float32)}\n",
      "sample dict keys: dict_keys(['start', 'target', 'item_id', 'feat_dynamic_real'])\n",
      "start: 2018-01-01 00:00 <class 'pandas._libs.tslibs.period.Period'>\n",
      "target shape: <class 'numpy.ndarray'> (25356,)\n",
      "item id: 0\n",
      "feat_dynamic_real\": <class 'numpy.ndarray'> (3, 25357) float32\n",
      "220000\n",
      "sample snippet: {'start': Period('2018-01-18 14:36', 'T'), 'target': array([8.339882]), 'item_id': 0, 'feat_dynamic_real': array([[1.7997806e+06, 1.7997806e+06],\n",
      "       [9.9999303e-01, 9.9999303e-01],\n",
      "       [1.5224874e+07, 1.5224874e+07]], dtype=float32)}\n",
      "sample dict keys: dict_keys(['start', 'target', 'item_id', 'feat_dynamic_real'])\n",
      "start: 2018-01-18 14:36 <class 'pandas._libs.tslibs.period.Period'>\n",
      "target shape: <class 'numpy.ndarray'> (1,)\n",
      "item id: 0\n",
      "feat_dynamic_real\": <class 'numpy.ndarray'> (3, 2) float32\n"
     ]
    }
   ],
   "source": [
    "test_sample = next(test_data_input_dataset_iter)\n",
    "print(len(test_data_input_dataset))\n",
    "print_data_sample(test_sample)\n",
    "\n",
    "test_sample = next(test_data_label_dataset_iter)\n",
    "print(len(test_data_label_dataset))\n",
    "print_data_sample(test_sample)\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "test_sample = next(test_data_input_dataset_iter)\n",
    "print(len(test_data_input_dataset))\n",
    "print_data_sample(test_sample)\n",
    "\n",
    "test_sample = next(test_data_label_dataset_iter)\n",
    "print(len(test_data_label_dataset))\n",
    "print_data_sample(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a31982b7-0464-45ae-9fde-0fdf5f3a838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:17</th>\n",
       "      <td>2713164.75</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>24343336.0</td>\n",
       "      <td>-2.570152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:18</th>\n",
       "      <td>2263878.50</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>24792622.0</td>\n",
       "      <td>-5.270243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:19</th>\n",
       "      <td>3197117.00</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>-7.500052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:20</th>\n",
       "      <td>3197117.00</td>\n",
       "      <td>0.999193</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>-4.699826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:21</th>\n",
       "      <td>3269983.25</td>\n",
       "      <td>0.999193</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>-5.300045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:22</th>\n",
       "      <td>3269983.25</td>\n",
       "      <td>0.999193</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>1.100302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:23</th>\n",
       "      <td>3269983.25</td>\n",
       "      <td>0.998842</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19 08:24</th>\n",
       "      <td>3391427.50</td>\n",
       "      <td>0.998842</td>\n",
       "      <td>24907852.0</td>\n",
       "      <td>-1.369715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  imbalance_size  reference_price  matched_size    target\n",
       "timestamp                                                                \n",
       "2018-01-19 08:17      2713164.75         0.999368    24343336.0 -2.570152\n",
       "2018-01-19 08:18      2263878.50         0.999368    24792622.0 -5.270243\n",
       "2018-01-19 08:19      3197117.00         0.999368    24907852.0 -7.500052\n",
       "2018-01-19 08:20      3197117.00         0.999193    24907852.0 -4.699826\n",
       "2018-01-19 08:21      3269983.25         0.999193    24907852.0 -5.300045\n",
       "2018-01-19 08:22      3269983.25         0.999193    24907852.0  1.100302\n",
       "2018-01-19 08:23      3269983.25         0.998842    24907852.0  0.979900\n",
       "2018-01-19 08:24      3391427.50         0.998842    24907852.0 -1.369715"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[raw_df[\"stock_id\"] == 0].iloc[-38:-30][feat_dynamic_real + [\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbbfb6a5-1caa-4147-a96d-afa35f177452",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1706407187985,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "20fe036e"
   },
   "outputs": [],
   "source": [
    "def create_transformation() -> Transformation:\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        []\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        # [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.ITEM_ID,\n",
    "                    expected_ndim=0,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            # if config.num_static_categorical_features > 0\n",
    "            # else []\n",
    "        )\n",
    "        + (\n",
    "            # [\n",
    "            #     AsNumpyArray(\n",
    "            #         field=FieldName.FEAT_STATIC_REAL,\n",
    "            #         expected_ndim=1,\n",
    "            #     )\n",
    "            # ]\n",
    "            # if config.num_static_real_features > 0\n",
    "            # else []\n",
    "            []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            # AddObservedValuesIndicator(\n",
    "            #     target_field=FieldName.TARGET,\n",
    "            #     output_field=FieldName.OBSERVED_VALUES,\n",
    "            # ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            # AddTimeFeatures(\n",
    "            #     start_field=FieldName.START,\n",
    "            #     target_field=FieldName.TARGET,\n",
    "            #     output_field=FieldName.FEAT_TIME,\n",
    "            #     time_features=time_features_from_frequency_str(freq),\n",
    "            #     pred_length=config.prediction_length,\n",
    "            # ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            # AddAgeFeature(\n",
    "            #     target_field=FieldName.TARGET,\n",
    "            #     output_field=FieldName.FEAT_AGE,\n",
    "            #     pred_length=config.prediction_length,\n",
    "            #     log_scale=False,\n",
    "            # ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    # if config.num_dynamic_real_features > 0\n",
    "                    # else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.ITEM_ID: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86fb2f8e-dcde-4a92-8c38-0f4828466edc",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706407187985,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "cae7600d"
   },
   "outputs": [],
   "source": [
    "def create_instance_splitter(\n",
    "    prediction_length: int,\n",
    "    context_length: int,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=context_length,\n",
    "        future_length=prediction_length,\n",
    "        time_series_fields=[\"time_features\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b88dea0-4d23-425d-abd3-694b40a7dfb1",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706407187985,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "6995101c"
   },
   "outputs": [],
   "source": [
    "def create_train_dataloader(\n",
    "    prediction_length: int,\n",
    "    context_length: int,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    cache_data: bool = True,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"future_time_features\",\n",
    "        \"static_categorical_features\",\n",
    "    ]\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation()\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "    if cache_data:\n",
    "        transformed_data = Cached(transformed_data)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(prediction_length, context_length, \"train\")\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    stream = Cyclic(transformed_data).stream()\n",
    "    training_instances = instance_splitter.apply(stream)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_length=shuffle_buffer_length,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b845295c-c29d-4517-81a9-1f3e98a03842",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706407187985,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "10c55455"
   },
   "outputs": [],
   "source": [
    "def create_backtest_dataloader(\n",
    "    prediction_length: int,\n",
    "    context_length: int,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"future_time_features\",\n",
    "        \"static_categorical_features\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation()\n",
    "    transformed_data = transformation.apply(data)\n",
    "\n",
    "    # We create a Validation Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(prediction_length, context_length, \"validation\")\n",
    "\n",
    "    # we apply the transformations in train mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb011aec-49ab-4c0b-8a54-a79923bcbffa",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1706407187985,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "71459c61-949b-4ff6-91af-5a636839aa60"
   },
   "outputs": [],
   "source": [
    "def create_test_dataloader(\n",
    "    prediction_length: int,\n",
    "    context_length: int,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"future_time_features\",\n",
    "        \"static_categorical_features\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation()\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # We create a test Instance splitter to sample the very last\n",
    "    # context window from the dataset provided.\n",
    "    instance_sampler = create_instance_splitter(prediction_length, context_length, \"test\")\n",
    "\n",
    "    # We apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fb395dd-6c1b-4cc5-b6a0-778369e24ffa",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706407187986,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "20e2338b"
   },
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=seq_len,\n",
    "    data=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "test_dataloader = create_backtest_dataloader(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=seq_len,\n",
    "    data=test_data_input_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f5df2f3-c448-4606-9bd3-b52e0bbbe918",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706407187986,
     "user": {
      "displayName": "Man Chau",
      "userId": "01327881085848547519"
     },
     "user_tz": -480
    },
    "id": "YU2h9OOB5IsX",
    "outputId": "5a7f5e13-46d9-4c9c-d271-78da16b8e7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_features torch.Size([256, 55, 3]) torch.FloatTensor\n",
      "past_values torch.Size([256, 55]) torch.FloatTensor\n",
      "future_time_features torch.Size([256, 1, 3]) torch.FloatTensor\n",
      "static_categorical_features torch.Size([256]) torch.LongTensor\n",
      "future_values torch.Size([256, 1]) torch.FloatTensor\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())\n",
    "print(len(list(iter(train_dataloader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a11c52e3-2507-4a83-b6aa-7c9d1ffa2e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_features torch.Size([64, 55, 3]) torch.FloatTensor\n",
      "past_values torch.Size([64, 55]) torch.FloatTensor\n",
      "future_time_features torch.Size([64, 1, 3]) torch.FloatTensor\n",
      "static_categorical_features torch.Size([64]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "test_sample_batch = next(iter(test_dataloader))\n",
    "for k, v in test_sample_batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143dab39-d000-4acc-ba6d-230fb7e9eb76",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a795a422-fa6a-4195-961b-2c0b9e712f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df_grouped)\n",
    "embedding_dim = 4\n",
    "d_model = 32\n",
    "nhead = 4\n",
    "d_hid = 32\n",
    "nlayers = 2\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98310dcf-a011-472d-a79a-e7738cb71c06",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4374a57-0a89-45d8-9adb-e3ccbc2eb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    num_input_features,\n",
    "    num_classes,\n",
    "    embedding_dim,\n",
    "    d_model,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    dropout,\n",
    "    NoopModelLogger(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdca2b6f-b702-4f41-b720-58cbb99ed5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "validation_criterion = nn.L1Loss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8355c-807b-431d-88e8-ba5c53b36769",
   "metadata": {},
   "source": [
    "## Test model with 2 identical samples, test model is non-random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78c880f2-1332-41ad-9b1f-8702d4eb513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 55, 3]) torch.Size([2]) tensor([0, 0])\n",
      "test_model_with_2_identical_samples - torch.Size([2, 55, 7])\n",
      "test_model_with_2_identical_samples - input_ff - torch.Size([2, 55, 4])\n",
      "test_model_with_2_identical_samples - src_mask - torch.Size([55, 55])\n",
      "test_model_with_2_identical_samples - encoder - torch.Size([2, 55, 4])\n",
      "test_model_with_2_identical_samples - final_linear - torch.Size([2, 55, 1])\n",
      "test_model_with_2_identical_samples - output - torch.Size([2])\n",
      "torch.Size([2]) tensor([-0.0099, -0.0099], grad_fn=<SqueezeBackward1>) torch.Size([2, 55]) torch.Size([2]) tensor([9.7597, 9.7597], dtype=torch.float64)\n",
      "tensor(9.7696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_model = TransformerModel(\n",
    "    num_input_features,\n",
    "    num_classes,\n",
    "    4,\n",
    "    4,\n",
    "    2,\n",
    "    4,\n",
    "    2,\n",
    "    dropout,\n",
    "    BasicModelLogger(\"test_model_with_2_identical_samples\"),\n",
    ")\n",
    "sample_model.eval()\n",
    "sample_input_features = training_sample_batch[0][0]\n",
    "sample_input_features = sample_input_features.expand(2, -1, -1)\n",
    "sample_input_item_id = training_sample_batch[1][0]\n",
    "sample_input_item_id = sample_input_item_id.expand(2)\n",
    "print(sample_input_features.size(), sample_input_item_id.size(), sample_input_item_id)\n",
    "sample_output = sample_model(sample_input_features, sample_input_item_id)\n",
    "sample_targets = training_sample_batch[2][0]\n",
    "sample_targets = sample_targets.expand(2, -1)\n",
    "sample_actual_targets = sample_targets[:, -1]\n",
    "print(sample_output.size(), sample_output, sample_targets.size(), sample_actual_targets.size(), sample_actual_targets)\n",
    "sample_loss = criterion(sample_output, sample_actual_targets)\n",
    "print(sample_loss)\n",
    "del sample_model, sample_input_features, sample_input_item_id, sample_output, sample_targets, sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570728ef-07e3-4b92-82fc-1745ef9962aa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22350a63-76bd-47cb-baf5-8ec579fafb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82221843-d155-49cd-9ce1-c784fe07065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57d4c816-e9db-434c-a728-47e8499ca89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = accelerator.prepare(model, optimizer)\n",
    "training_dataloader, validation_dataloader = accelerator.prepare(training_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8d79ddb-b6aa-4f3c-a930-265549a10ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ab39740-a848-4844-ad63-8e52e9c950c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 861\n"
     ]
    }
   ],
   "source": [
    "exp_training_num_batches = math.ceil(len(training_sampler) / training_batch_size)\n",
    "exp_validation_num_batches = math.ceil(len(validation_sampler) / validation_batch_size)\n",
    "print(exp_training_num_batches, exp_validation_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86e3ef67-6d6f-47b6-9035-219b6db0e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir_path = \"transformer_encoder_checkpoints\"\n",
    "checkpoint_prefix = \"20240313_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4e105f6-2835-425f-993c-96e40a1d8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:15<00:00,  6.36it/s]\n",
      "Validation epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 861/861 [00:51<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - training loss: 4.463226436177743, validation loss: 5.7897948037601505 / (220200,), checkpoint_path: transformer_encoder_checkpoints/20240313_test_epoch0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:15<00:00,  6.27it/s]\n",
      "Validation epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 861/861 [00:58<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - training loss: 4.461385493080297, validation loss: 5.790021116168703 / (220200,), checkpoint_path: transformer_encoder_checkpoints/20240313_test_epoch1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, training_dataloader, epoch, criterion):\n",
    "    training_iter = enumerate(training_dataloader)\n",
    "    training_iter = tqdm(training_iter, desc=f\"Training epoch {epoch}\", total=exp_training_num_batches)\n",
    "\n",
    "    total_training_loss = 0.0\n",
    "    training_batch_cnt = 0\n",
    "\n",
    "    for idx, batch in training_iter:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features, item_id, targets = batch[0], batch[1], batch[2]\n",
    "        actual_targets = targets[:, -1]\n",
    "\n",
    "        output = model(features, item_id)\n",
    "\n",
    "        loss = criterion(output, actual_targets)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_training_loss += loss.item()\n",
    "        training_batch_cnt += 1\n",
    "\n",
    "    avg_training_loss = total_training_loss / training_batch_cnt if training_batch_cnt > 0 else np.nan\n",
    "    return avg_training_loss\n",
    "\n",
    "\n",
    "def validation_epoch(model, validation_dataloader, epoch, criterion):\n",
    "    validation_iter = enumerate(validation_dataloader)\n",
    "    validation_iter = tqdm(validation_iter, desc=f\"Validation epoch {epoch}\", total=exp_validation_num_batches)\n",
    "    validation_losses = None\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in validation_iter:\n",
    "            features, item_id, targets = batch[0], batch[1], batch[2]\n",
    "            actual_targets = targets[:, -1]\n",
    "            output = model(features, item_id)\n",
    "            loss = criterion(output, actual_targets)\n",
    "            loss = loss.cpu().numpy()\n",
    "            if validation_losses is None:\n",
    "                validation_losses = loss\n",
    "            else:\n",
    "                validation_losses = np.concatenate((validation_losses, loss))\n",
    "    validation_avg_loss = np.mean(validation_losses)\n",
    "    return validation_avg_loss, validation_losses\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    avg_training_loss = train_epoch(model, training_dataloader, epoch, criterion)\n",
    "\n",
    "    model.eval()\n",
    "    validation_avg_loss, validation_losses = validation_epoch(model, validation_dataloader, epoch, validation_criterion)\n",
    "\n",
    "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    checkpoint_dict = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'avg_training_loss': avg_training_loss,\n",
    "        'validation_avg_loss': validation_avg_loss,\n",
    "    }\n",
    "    checkpoint_path = path.join(checkpoint_dir_path, f\"{checkpoint_prefix}_epoch{epoch}.pt\")\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch} - training loss: {avg_training_loss}, validation loss: {validation_avg_loss} / {validation_losses.shape}, checkpoint_path: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139bfba-7fca-482a-bf5c-0f633b61fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
